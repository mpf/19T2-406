\documentclass{article}
\usepackage{tabularx,fullpage,url}
\usepackage[top=1in, bottom=1in, left=.5in, right=.75in]{geometry}
\usepackage{amsmath,amssymb,graphicx,amsthm,xparse, color, mathrsfs} 
\usepackage{ epstopdf, fullpage}


\newcommand{\showsolution}[1]{}%{\textbf{Ans.}\;#1}

\newcommand{\mypagebreak}{\begin{center}
		\noindent\makebox[\linewidth]{\rule{7.5in}{1pt}}
	\end{center}}
\bibliographystyle{siam}
\newcommand{\minimize}[1]{\underset{#1}{\mathrm{minimize}}}
\newcommand{\maximize}[1]{\underset{#1}{\mathrm{maximize}}}
\newcommand{\mini}[1]{\underset{#1}{\mathrm{min}}}
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}}
\newcommand{\st}{\mathrm{subject\; to}}
\newcommand{\rank}{\mathbf{rank}}
\newcommand{\diag}{\mathbf{diag}}
\newcommand{\sign}{\mathbf{sign}}
\newcommand{\mb}{\mathbf}
\renewcommand{\Re}{\mathbf R}
\newcommand{\mL}{\mathcal L}
\newcommand{\mC}{\mathcal C}
\newcommand{\mB}{\mathcal B}
\newcommand{\mS}{\mathcal S}
\newcommand{\tr}{\mathbf{tr}}

\newcommand{\proj}{\mathbf{proj}}
\newcommand{\range}{\mathbf{range}}
\newcommand{\vnull}{\mathbf{null}}

\newcommand{\bmat}{\left[\begin{matrix}}
\newcommand{\emat}{\end{matrix}\right]}

\newcommand{\red}[1]{{\color{red}#1}}

\newcommand{\gray}[1]{\textcolor{lightgray}{#1}}
\begin{document}
{\Large\textbf{CPSC 406: Midterm practice questions \hfill }}


\mypagebreak



\begin{enumerate}
	\item \textbf{Cholesky factorization.} 
	
	\begin{enumerate}
		\item Show the steps used to compute the Cholesky factorization $A = LL^T$ of 
		\[
		A = \bmat 1 + \epsilon_1 & 1 \\ 1 & 1 + \epsilon_2 \emat.
		\]
		Discuss what happens if either $\epsilon_1\to 0$ or $\epsilon_2 \to 0$
		
		\item Also, perform the Cholesky decomposition of 
		\[
		A = \bmat 0 & 1 \\ 1 &  0 \emat.
		\]
		Describe why this is impossible, and why that makes sense. 
	\end{enumerate}
	\item Show that $A = \bmat a& -a \\ -a &a\emat$ is positive semidefinite, but not positive definite.
	\item Consider the block diagonal matrix 
			$$A = \bmat B&0\\0&C\emat.$$
			Suppose that $B\succ	 0$ and $C\succ 0$. Show that this implies $A$ is indefinite.
	\item Suppose that $x$ and $\hat{x}$ both are optimal variables to the least squares problem.
			$$\minimize{x}\ \|Ax-b\|_2^2.$$
			Show that this implies $x-\hat{x}$ is in the null space of $A$.
	\item Consider the matrix and vector 
			$$A = \bmat I&I\\I&I\emat, \ I =\bmat 1&0\\0&1\emat,\ x = \bmat x_1\\x_2\\x_3\\x_4\emat.$$
			Decompose $x = u+v$ where $u$ is in the range of space of $A$ and $v^\intercal u = 0$.
	\item Beck 2.17
	\item Suppose that $f(x) = x^\intercal Ax$ where $A = \bmat 1&0\\0&2 \emat$. Suppose $t=3$, and run gradient descent
			$$x^+ = x - t\nabla f(x).$$
			For what choice of $x$ will this diverge? For what choice of $t$ will this converge regardless of $x$?			
	\item \textbf{Gradient descent} Consider the minimization problem
	\[
	\minimize{x\in \Re^n} \quad f(x) := \frac{1}{2} x^TAx 
	\]
	where $A$ is symmetric positive semidefinite with largest eigenvalue / eigenvector pair $\lambda_{\max}>0$, $u_{\max}$; that is, 
	\[
	Au_{\max} = \lambda_{\max} u_{\max} \quad \text{ and } \quad u_{\max}^TAu_{\max} = \max_{\|u\|_2=1} u^TAu = \lambda_{\max}.
	\]
	We now consider gradient descent on this objective
	\[
	x^{(k+1)} = x^{(k)} - t \nabla f(x^{(k)})
	\]
where $x^{(k)}$ is the variable at iteration $k$, and $x^{(k+1)}$ is the variable at iteration $k+1$ (after 1 gradient step).
	
	\begin{enumerate}
		\item Write the gradient of $f$ at $x$. 
		\item Recall that $f$ is $L$-smooth if 
		\[
		\|\nabla f(x) - \nabla f(y)\|_2 \leq L\|x-y\|_2, \quad \forall x,y.
		\]
		What is $L$ for $f(x) = \frac{1}{2} x^TAx$?
		\item The \emph{amount of descent} can be characterized as 
		\[
		f(x^{(k)}) - f(x^{(k+1)}) = \frac{1}{2} (x^{(k)})^TAx^{(k)} - \frac{1}{2} (x^{(k)}-t\nabla f(x^{(k)}))^TA(x^{(k)}-t\nabla f(x^{(k)})).
		\]
		Expand and simplify the right hand side. In particular, find $c_1$ and $c_2$ where 
		\[
		f(x^{(k)}) - f(x^{(k+1)}) = c_1 \nabla f(x^{(k)})^T\nabla f(x^{(k)}) + c_2 \nabla f(x^{(k)})^TA\nabla f(x^{(k)}).
		\]
		\item Explain why if $0 < t < 2/L$ and $x^{(k)}$ is not a stationary point, then $f(x^{(k)}) - f(x^{(k+1)}) > 0$ for any $x^{(k)}$.
		\item Now suppose $t > 2/L$. Give a direction $u$ and show that for this choice of $t$ and $u$, with $x^{(k+1)} = x^{(k)} - tu$, then
		\[
		f(x^{(k+1)})   > f(x^{(k)}).
		\]
		\item Explain in 1-3 sentences why, when using gradient descent with constant step size on this objective, the recommendation is to have $t < 2/L$.
	\end{enumerate}
\end{enumerate}

\end{document}