{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Computational Optimization \u00b6 This course covers the main algorithms for continuous optimization, including unconstrained and constrained problems, large-scale problems, and duality theory and sensitivity. We will also cover numerical linear algebra operations needed in optimization, including LU, QR, and Cholesky decompositions. Discrete optimization problems are not covered. Lectures \u00b6 Monday, Wednesday, and Friday, 2-3 pm, DMP 110 Teaching staff (2019 Term 2) \u00b6 Co-instructor: Michael P. Friedlander . Office hours: Monday, 3-4p (ICCS X150) Co-instructor: Babhru Joshi. Office hours: Friday, 3-4p (ICCS X150, table 2) Teaching assistant: Zhenan Fan. Office hours: Tuesday, 2-3p (ICCS X150, table 6) Teaching Assistant: Huang Fang. Office hours: Thursday 2-3p (ICCS X150, table 4) Textbook \u00b6 Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MATLAB , Amir Beck (SIAM, 2014). This book is available online through the UBC Library. Course requirements \u00b6 One of CPSC 302, CPSC 303, or MATH 307. Course discussion board \u00b6 Discussion board is hosted on Piazza . Here is the link to enroll.","title":"Home Page"},{"location":"#computational-optimization","text":"This course covers the main algorithms for continuous optimization, including unconstrained and constrained problems, large-scale problems, and duality theory and sensitivity. We will also cover numerical linear algebra operations needed in optimization, including LU, QR, and Cholesky decompositions. Discrete optimization problems are not covered.","title":"Computational Optimization"},{"location":"#lectures","text":"Monday, Wednesday, and Friday, 2-3 pm, DMP 110","title":"Lectures"},{"location":"#teaching-staff-2019-term-2","text":"Co-instructor: Michael P. Friedlander . Office hours: Monday, 3-4p (ICCS X150) Co-instructor: Babhru Joshi. Office hours: Friday, 3-4p (ICCS X150, table 2) Teaching assistant: Zhenan Fan. Office hours: Tuesday, 2-3p (ICCS X150, table 6) Teaching Assistant: Huang Fang. Office hours: Thursday 2-3p (ICCS X150, table 4)","title":"Teaching staff (2019 Term 2)"},{"location":"#textbook","text":"Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MATLAB , Amir Beck (SIAM, 2014). This book is available online through the UBC Library.","title":"Textbook"},{"location":"#course-requirements","text":"One of CPSC 302, CPSC 303, or MATH 307.","title":"Course requirements"},{"location":"#course-discussion-board","text":"Discussion board is hosted on Piazza . Here is the link to enroll.","title":"Course discussion board"},{"location":"grades/","text":"Grades and policies \u00b6 Grade distribution \u00b6 assignments (6): 30% midterm exam: 30% final exam: 40% Collaboration \u00b6 Most homeworks involve programming tasks. You may collaborate and consult with other students in the course, but you must hand in your own assignments and your own code. If you have collaborated or consulted with someone while working on your assignment, you must acknowledge this explicitly in your submitted assignment. If you are unsure about any of these rules, feel free to consult with your instructor or visit the departmental webpage on Collaboration and Plagiarism . Late submissions \u00b6 Each student has a three business-day allowance to use throughout the term. If an assignment is due by Thursday, then submission by Friday counts as one delay; a submission by Monday counts as two delays; a submission by Tuesday counts as three delays (and consumes the entire three-day allowance). Apart from using delays, late submissions are not accepted. Once a solution set has been posted, no more late submissions are permitted; consequently, you may not always be able to use all of your delays. Policies \u00b6 No makeup exam for the midterm of final. If you missed the midterm exam you must document a justification. Midterm exam grade will not be counted if it is lower than your final exam grade. To pass the course you must do the assigned coursework, write the midterm and final exams, pass the final exam, and obtain an overall pass average according to the grading scheme. The instructors reserve the right to modify the grading scheme at any time.","title":"Grades"},{"location":"grades/#grades-and-policies","text":"","title":"Grades and policies"},{"location":"grades/#grade-distribution","text":"assignments (6): 30% midterm exam: 30% final exam: 40%","title":"Grade distribution"},{"location":"grades/#collaboration","text":"Most homeworks involve programming tasks. You may collaborate and consult with other students in the course, but you must hand in your own assignments and your own code. If you have collaborated or consulted with someone while working on your assignment, you must acknowledge this explicitly in your submitted assignment. If you are unsure about any of these rules, feel free to consult with your instructor or visit the departmental webpage on Collaboration and Plagiarism .","title":"Collaboration"},{"location":"grades/#late-submissions","text":"Each student has a three business-day allowance to use throughout the term. If an assignment is due by Thursday, then submission by Friday counts as one delay; a submission by Monday counts as two delays; a submission by Tuesday counts as three delays (and consumes the entire three-day allowance). Apart from using delays, late submissions are not accepted. Once a solution set has been posted, no more late submissions are permitted; consequently, you may not always be able to use all of your delays.","title":"Late submissions"},{"location":"grades/#policies","text":"No makeup exam for the midterm of final. If you missed the midterm exam you must document a justification. Midterm exam grade will not be counted if it is lower than your final exam grade. To pass the course you must do the assigned coursework, write the midterm and final exams, pass the final exam, and obtain an overall pass average according to the grading scheme. The instructors reserve the right to modify the grading scheme at any time.","title":"Policies"},{"location":"schedule/","text":"Course schedule \u00b6 This is a tentative schedule. It will change often. Lecture Date / Day Topic Notes Homework 1 06 Jan / Mon Introduction 2 08 Jan / Wed Least squares notebook 3 10 Jan / Fri QR factorization hw1 out 4 13 Jan / Mon Regularized least squares notebook 15 Jan / Wed no lecture 5 17 Jan / Fri Non-linear least squares notebook hw1 due at 6pm 6 20 Jan / Mon Unconstrained optimization hw2 out 7 22 Jan / Wed Unconstrained optimization 8 24 Jan / Fri Quadratic functions 9 27 Jan / Mon Quadratic functions hw2 due at 6pm 10 29 Jan / Wed Descent methods 11 31 Jan / Fri Descent methods 12 3 Feb / Mon Newton's methods 13 5 Feb / Wed Quasi-Newton's methods 14 7 Feb / Fri 15 10 Feb / Mon Case study 16 12 Feb / Wed Review 17 14 Feb / Fri Midterm 17 Feb / Mon Holiday (Family Day) 19 Feb / Wed no lecture (spring break) 21 Feb / Fri no lecture (spring break) 18 24 Feb / Mon Convex sets 19 26 Feb / Wed Convex functions 20 28 Feb / Fri Constrained optimization 21 2 Mar / Mon Constrained optimization 22 4 Mar / Wed Linear programing 23 6 Mar / Fri Linear programing 24 9 Mar / Mon Simplex method 25 11 Mar / Wed Simplex method 26 13 Mar / Fri Simplex method 27 16 Mar / Mon Duality 28 18 Mar / Wed Duality 29 20 Mar / Fri Interior point methods 30 23 Mar / Mon Interior point methods 31 25 Mar / Wed 32 27 Mar / Fri 33 30 Mar / Mon 34 1 Apr / Wed 35 3 Apr / Fri 36 6 Apr / Mon 37 8 Apr / Wed","title":"Schedule"},{"location":"schedule/#course-schedule","text":"This is a tentative schedule. It will change often. Lecture Date / Day Topic Notes Homework 1 06 Jan / Mon Introduction 2 08 Jan / Wed Least squares notebook 3 10 Jan / Fri QR factorization hw1 out 4 13 Jan / Mon Regularized least squares notebook 15 Jan / Wed no lecture 5 17 Jan / Fri Non-linear least squares notebook hw1 due at 6pm 6 20 Jan / Mon Unconstrained optimization hw2 out 7 22 Jan / Wed Unconstrained optimization 8 24 Jan / Fri Quadratic functions 9 27 Jan / Mon Quadratic functions hw2 due at 6pm 10 29 Jan / Wed Descent methods 11 31 Jan / Fri Descent methods 12 3 Feb / Mon Newton's methods 13 5 Feb / Wed Quasi-Newton's methods 14 7 Feb / Fri 15 10 Feb / Mon Case study 16 12 Feb / Wed Review 17 14 Feb / Fri Midterm 17 Feb / Mon Holiday (Family Day) 19 Feb / Wed no lecture (spring break) 21 Feb / Fri no lecture (spring break) 18 24 Feb / Mon Convex sets 19 26 Feb / Wed Convex functions 20 28 Feb / Fri Constrained optimization 21 2 Mar / Mon Constrained optimization 22 4 Mar / Wed Linear programing 23 6 Mar / Fri Linear programing 24 9 Mar / Mon Simplex method 25 11 Mar / Wed Simplex method 26 13 Mar / Fri Simplex method 27 16 Mar / Mon Duality 28 18 Mar / Wed Duality 29 20 Mar / Fri Interior point methods 30 23 Mar / Mon Interior point methods 31 25 Mar / Wed 32 27 Mar / Fri 33 30 Mar / Mon 34 1 Apr / Wed 35 3 Apr / Fri 36 6 Apr / Mon 37 8 Apr / Wed","title":"Course schedule"},{"location":"homework/","text":"Homework submissions \u00b6 Here are some guidelines for homework submissions: Solutions must be submitted electronically, using the Canvas system. Solutions should be typeset. No handwritten solutions! You have several options available to you, include PDF files prepared using LaTeX and Jupyter notebooks. Jupyter notebook submissions are especially encouraged because these are a convenient way of packaging typeset solutions and corresponding results and code. If you choose to use a Jupyter notebook, please submit a PDF and the ipynb file. Adhere to the policy on collaboration and late submissions .","title":"Submissions"},{"location":"homework/#homework-submissions","text":"Here are some guidelines for homework submissions: Solutions must be submitted electronically, using the Canvas system. Solutions should be typeset. No handwritten solutions! You have several options available to you, include PDF files prepared using LaTeX and Jupyter notebooks. Jupyter notebook submissions are especially encouraged because these are a convenient way of packaging typeset solutions and corresponding results and code. If you choose to use a Jupyter notebook, please submit a PDF and the ipynb file. Adhere to the policy on collaboration and late submissions .","title":"Homework submissions"},{"location":"homework/hw1/hw1/","text":"Homework 1 (Due Jan 17, 6pm) \u00b6 Backsolve. Here, we will explore the computational complexity of solving the system \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} when \\mR \\mR is either upper triangular ( R_{ij} = 0 R_{ij} = 0 whenever i > j i > j ) or lower triangular ( R_{ij} = 0 R_{ij} = 0 whenever i < j i < j ). If \\mR \\mR were fully dense, then solving this system takes O(n^3) O(n^3) flops. We will show that when \\mR \\mR is upper or lower triangular, this system takes O(n^2) O(n^2) flops. Assume that the diagonal elements |R_{ii}| > \\epsilon |R_{ii}| > \\epsilon for \\epsilon \\epsilon suitably large in all cases. Consider \\mR \\mR lower triangular, e.g. we solve the system \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_1 x_1 . (This should take O(1) O(1) flops.) Given x_1,...,x_i x_1,...,x_i , show how to find x_{i+1} x_{i+1} . (This should take O(i) O(i) flops.) Putting it all together, we get O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} Now consider \\mR \\mR upper triangular, e.g. we solve the system \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_n x_n . (This should take O(1) O(1) flops.) Given x_{i+1},...,x_n x_{i+1},...,x_n , show how to find x_{i} x_{i} . (This should take O(n-i) O(n-i) flops.) Putting it all together, we get O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} Linear data fit. Download data (Here is the csv format of the same data ). Fit the best line f(z) = x_1 + x_2z f(z) = x_1 + x_2z to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) ; that is, find the best approximation of the line f(z) f(z) to y y in the 2-norm sense. Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. Polynomial data fit. Using the same data as above, fit the best order- d d polynomial to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) , for d = 2,3,4,5 d = 2,3,4,5 . That is, find x_1,...,x_{d+1} x_1,...,x_{d+1} such that f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d best approximates the data in the 2-norm sense (minimizing \\sum_i (f(z_i)-y_i)^2 \\sum_i (f(z_i)-y_i)^2 ). Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. About how many degrees is needed for a reasonable fit? QR factorization. Consider a full rank, underdetermined but consistent linear system \\mA\\vx = \\vb \\mA\\vx = \\vb , where \\mA \\mA is m\\times n m\\times n with m < n m < n . Show how to use the QR factorization to obtain a soution of this system. The following script can be used to generate random matrices in Julia, given dimensions m=10 m=10 and n=20 n=20 : m = 10 n = 20 A = randn ( m , n ) x = randn ( n ) b = A * x Write a Julia code for solving for x x using the procedure outlined in the previous part of the question. Record the runtime using the Julia call time . (Make sure you are not running anything else or it will interfere with the timing results.) Record the runtimes for matrices of sizes (m,n) = (10,20) (m,n) = (10,20) , (100,200) (100,200) , (100,2000) (100,2000) , (100,20000) (100,20000) , and (100,200000) (100,200000) . Compare the runtimes against finding x x using \\vx = \\mA\\backslash \\vb \\vx = \\mA\\backslash \\vb . The underdetermined and consistent linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has infinitely many solutions. For the case where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} is full rank, show how to use the QR factorization to obtain the least norm solution, i.e. find \\vx_{LN} \\vx_{LN} that solves \\min_{\\vx\\in\\R^n}\\quad \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb \\min_{\\vx\\in\\R^n}\\quad \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb using QR decomposition.","title":"Homework 1"},{"location":"homework/hw1/hw1/#homework-1-due-jan-17-6pm","text":"Backsolve. Here, we will explore the computational complexity of solving the system \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} when \\mR \\mR is either upper triangular ( R_{ij} = 0 R_{ij} = 0 whenever i > j i > j ) or lower triangular ( R_{ij} = 0 R_{ij} = 0 whenever i < j i < j ). If \\mR \\mR were fully dense, then solving this system takes O(n^3) O(n^3) flops. We will show that when \\mR \\mR is upper or lower triangular, this system takes O(n^2) O(n^2) flops. Assume that the diagonal elements |R_{ii}| > \\epsilon |R_{ii}| > \\epsilon for \\epsilon \\epsilon suitably large in all cases. Consider \\mR \\mR lower triangular, e.g. we solve the system \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_1 x_1 . (This should take O(1) O(1) flops.) Given x_1,...,x_i x_1,...,x_i , show how to find x_{i+1} x_{i+1} . (This should take O(i) O(i) flops.) Putting it all together, we get O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} Now consider \\mR \\mR upper triangular, e.g. we solve the system \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_n x_n . (This should take O(1) O(1) flops.) Given x_{i+1},...,x_n x_{i+1},...,x_n , show how to find x_{i} x_{i} . (This should take O(n-i) O(n-i) flops.) Putting it all together, we get O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} Linear data fit. Download data (Here is the csv format of the same data ). Fit the best line f(z) = x_1 + x_2z f(z) = x_1 + x_2z to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) ; that is, find the best approximation of the line f(z) f(z) to y y in the 2-norm sense. Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. Polynomial data fit. Using the same data as above, fit the best order- d d polynomial to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) , for d = 2,3,4,5 d = 2,3,4,5 . That is, find x_1,...,x_{d+1} x_1,...,x_{d+1} such that f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d best approximates the data in the 2-norm sense (minimizing \\sum_i (f(z_i)-y_i)^2 \\sum_i (f(z_i)-y_i)^2 ). Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. About how many degrees is needed for a reasonable fit? QR factorization. Consider a full rank, underdetermined but consistent linear system \\mA\\vx = \\vb \\mA\\vx = \\vb , where \\mA \\mA is m\\times n m\\times n with m < n m < n . Show how to use the QR factorization to obtain a soution of this system. The following script can be used to generate random matrices in Julia, given dimensions m=10 m=10 and n=20 n=20 : m = 10 n = 20 A = randn ( m , n ) x = randn ( n ) b = A * x Write a Julia code for solving for x x using the procedure outlined in the previous part of the question. Record the runtime using the Julia call time . (Make sure you are not running anything else or it will interfere with the timing results.) Record the runtimes for matrices of sizes (m,n) = (10,20) (m,n) = (10,20) , (100,200) (100,200) , (100,2000) (100,2000) , (100,20000) (100,20000) , and (100,200000) (100,200000) . Compare the runtimes against finding x x using \\vx = \\mA\\backslash \\vb \\vx = \\mA\\backslash \\vb . The underdetermined and consistent linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has infinitely many solutions. For the case where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} is full rank, show how to use the QR factorization to obtain the least norm solution, i.e. find \\vx_{LN} \\vx_{LN} that solves \\min_{\\vx\\in\\R^n}\\quad \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb \\min_{\\vx\\in\\R^n}\\quad \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb using QR decomposition.","title":"Homework 1 (Due Jan  17, 6pm)"},{"location":"homework/hw2/hw2/","text":"CPSC 406: Homework 2 (Due Jan 27, 6pm) \u00b6 (Beck 3.1) Let \\mA\\in \\R^{m\\times n} \\mA\\in \\R^{m\\times n} , \\vb\\in \\R^n \\vb\\in \\R^n , \\mL \\in \\R^{p\\times n} \\mL \\in \\R^{p\\times n} and scalar \\lambda >0 \\lambda >0 . Consider the regularized least squares problem \\mathop{\\text{minimize}}_{\\vx\\in \\R^n} \\quad f(\\vx) := \\|\\mA \\vx-\\vb\\|_2^2 + \\lambda \\|\\mL\\vx\\|_2^2. \\mathop{\\text{minimize}}_{\\vx\\in \\R^n} \\quad f(\\vx) := \\|\\mA \\vx-\\vb\\|_2^2 + \\lambda \\|\\mL\\vx\\|_2^2. Show that this problem has a unique solution if and only if \\vnull(\\mA) \\cap \\vnull(\\mL) = \\{\\vzero\\} \\vnull(\\mA) \\cap \\vnull(\\mL) = \\{\\vzero\\} . Hint: Look up De Morgan's laws. Multiobjective problems. Often, in real world applications, we wish to accomplish multiple goals at once. For example, we may wish to buy the biggest house with the least amount of money, or buy the most profitable stocks which also have least risk. These objectives are often competing, and it is impossible to optimize one without negatively impacting the other. 2-norm regularization. Consider the 2-norm regularized least squares problem \\minimize{\\vx\\in \\R^n} \\quad \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\frac{\\gamma}{2}\\underbrace{\\|\\vx\\|_2^2}_{f_2(\\vx)} \\minimize{\\vx\\in \\R^n} \\quad \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\frac{\\gamma}{2}\\underbrace{\\|\\vx\\|_2^2}_{f_2(\\vx)} Show that at optimality, \\|\\vx\\|_2^2 = \\sum_{i=1}^n \\frac{1}{(d_i+\\gamma)^2} g_i^2 \\|\\vx\\|_2^2 = \\sum_{i=1}^n \\frac{1}{(d_i+\\gamma)^2} g_i^2 where \\mA\\trans\\mA = \\mQ\\mD\\mQ\\trans \\mA\\trans\\mA = \\mQ\\mD\\mQ\\trans the eigenvalue decomposition and \\vg = \\mQ\\trans\\mA\\trans\\vb \\vg = \\mQ\\trans\\mA\\trans\\vb . Roughly sketch what the Pareto-Frontier looks like here, paying attention to the limiting behavior ( \\gamma\\to 0 \\gamma\\to 0 and \\gamma\\to +\\infty \\gamma\\to +\\infty ). Sparsity. As an example, let us consider the problem of sparse recovery, where we wish to find the sparsest solution \\vx \\vx such that \\mA\\vx \\approx \\vb \\mA\\vx \\approx \\vb . Here, we motivate sparsity by minimizing the 1-norm of \\vx \\vx , e.g. f_1(\\vx) = \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\qquad f_2(\\vx) = \\|\\vx\\|_1 f_1(\\vx) = \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\qquad f_2(\\vx) = \\|\\vx\\|_1 and the goal is to make both f_1(\\vx) f_1(\\vx) and f_2(\\vx) f_2(\\vx) small. Download data \\mA\\in \\R^{m\\times n} \\mA\\in \\R^{m\\times n} ( jld , csv ) and \\vb\\in \\R^m \\vb\\in \\R^m ( jld , csv ). Using Convex.jl or CVX package solve the following problem for \\gamma = 1 \\gamma = 1 . \\minimize{x\\in \\R^n} \\; \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\gamma \\underbrace{\\|\\vx\\|_1}_{f_2(\\vx)} \\minimize{x\\in \\R^n} \\; \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\gamma \\underbrace{\\|\\vx\\|_1}_{f_2(\\vx)} Report the accuracy ( f_1(\\vx^*) f_1(\\vx^*) value) and sparsity metric ( f_2(\\vx^*) f_2(\\vx^*) value). To show the effect of the \"sparsity promoter\" term ( f_2(\\vx) f_2(\\vx) ), repeat the above exercise for \\gamma = 0.01 \\gamma = 0.01 and \\gamma = 10 \\gamma = 10 . Plot both solutions on top of the original signal \\vx_0 \\vx_0 ( jld , csv ). Which solution gives better signal recovery? Repeat the above exercise for 100 different values of \\gamma \\gamma , generated from exp10.(range(-3, stop=3, length=100)) . Plot the cost f_1(\\vx) f_1(\\vx) on the x x -axis and f_2(\\vx) f_2(\\vx) on the y y -axis. This is the Pareto frontier . For any cost value pair to the upper right of this curve, at least one cost can be made smaller at no expense to the other. For any cost value pair to the lower left of the curve, the cost values are unattainable. The Pareto frontier represents the \"best case scenario\" set of points. We now consider another example where we motivate smoothness as well as accuracy. Download data \\mA\\in \\R^{m\\times n} \\mA\\in \\R^{m\\times n} ( jld , csv ) and \\vb\\in \\R^m \\vb\\in \\R^m ( jld , csv ). Again, using Convex.jl or CVX package, solve the following problem for \\gamma = 1 \\gamma = 1 . \\minimize{\\vx\\in \\R^n} \\; \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\gamma \\underbrace{\\|\\mD\\vx\\|_1}_{f_2(\\vx)} \\minimize{\\vx\\in \\R^n} \\; \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\gamma \\underbrace{\\|\\mD\\vx\\|_1}_{f_2(\\vx)} where \\mD = \\bmat 1 & -1 & 0 &\\cdots &0&0 \\\\ 0 &1 & -1 &\\cdots &0&0 \\\\ \\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & 1 & -1 \\emat \\in \\R^{n-1\\times n} \\mD = \\bmat 1 & -1 & 0 &\\cdots &0&0 \\\\ 0 &1 & -1 &\\cdots &0&0 \\\\ \\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & 1 & -1 \\emat \\in \\R^{n-1\\times n} Minimize this objective for \\gamma = 0.01 \\gamma = 0.01 and \\gamma = 10 \\gamma = 10 . Plot both solutions on top of the original signal \\vx_0 \\vx_0 ( jld , csv ). Which solutio)n gives better signal recovery? Repeat the above exercise for 100 different values of \\gamma \\gamma , generated from exp10.(range(-3, stop=3, length=100)) . Plot the Pareto frontier. Non-linear least squares (adapted from Beck 4.6) The source localization problem consists of minimizing \\minimize{\\vx\\in \\R^n} \\left\\{f(\\vx) := \\sum_{i=1}^m \\left(\\|\\vx-\\vc_i\\|_2^2 - d_i^2\\right)^2\\right\\} \\minimize{\\vx\\in \\R^n} \\left\\{f(\\vx) := \\sum_{i=1}^m \\left(\\|\\vx-\\vc_i\\|_2^2 - d_i^2\\right)^2\\right\\} for vectors \\vc_1,...,\\vc_m\\in \\R^n \\vc_1,...,\\vc_m\\in \\R^n and scalars d_1,...,d_m d_1,...,d_m . This is of course a nonlinear least squares problem, and thus the Gauss-Newton method can be employed to solve it. We will assume that n = 2 n = 2 . Compute the gradient of f(\\vx) f(\\vx) at a given point \\vx \\vx . Rewrite the problem as \\minimize{\\vx\\in \\R^n} \\quad \\|r(\\vx)\\|_2^2 \\minimize{\\vx\\in \\R^n} \\quad \\|r(\\vx)\\|_2^2 that is, identify r(\\vx) : \\R^n \\to \\R^m r(\\vx) : \\R^n \\to \\R^m . Compute also the Jacobian of r(\\vx) r(\\vx) J(\\vx) = \\bmat \\nabla r_1(\\vx)\\trans \\\\ \\vdots \\\\ \\nabla r_m(\\vx)\\trans\\emat. J(\\vx) = \\bmat \\nabla r_1(\\vx)\\trans \\\\ \\vdots \\\\ \\nabla r_m(\\vx)\\trans\\emat. Show that as long as all the points \\vc_1,...,\\vc_m \\vc_1,...,\\vc_m do not reside on the same line in the plane, and at each iterate \\vx^{(k)} \\neq \\vc_i \\vx^{(k)} \\neq \\vc_i for any i = 1,...,m i = 1,...,m , the method is well-defined, meaning that the linear least squares problem solved at each iteration has a unique solution. We will now implement some numerical methods on this problem. Download data \\mC\\in \\R^{2\\times 5} \\mC\\in \\R^{2\\times 5} ( jld , csv ), \\vd\\in \\R^5 \\vd\\in \\R^5 ( jld , csv ), and \\vx \\in \\R^2 \\vx \\in \\R^2 ( jld , csv ). The columns of the 2 \\times 5 2 \\times 5 matrix \\mC \\mC are the locations of the five sensors, \\vx \\vx is the \"true\" location of the source, and \\vd \\vd is the vector of noisy measurements between the source and the sensors. In all cases, initialize with \\vx^{(0)} = (1000, -500)\\trans \\vx^{(0)} = (1000, -500)\\trans . The gradient descent method operates via the iteration scheme \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)} \\nabla f(\\vx^{(k)}) \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)} \\nabla f(\\vx^{(k)}) for a choice of step size \\alpha^{(k)} \\alpha^{(k)} . Implement the gradient descent method for a constant step size \\alpha^{(k)} = \\bar \\alpha \\alpha^{(k)} = \\bar \\alpha . Try several values until you find the largest value of \\bar \\alpha \\bar \\alpha such that the method does not diverge. Hint: try several orders of magnitude from 10^{-10} 10^{-10} to 1 1 . Implement gradient descent with backtracking line search. Use parameters s=1, \\alpha=\\beta=0.5 s=1, \\alpha=\\beta=0.5 . (Refer also to page 51 in Beck .) Implement damped Gauss-Newton to minimize f(\\vx) f(\\vx) . Use the largest constant step size \\bar \\alpha \\leq 1 \\bar \\alpha \\leq 1 such that the method does not diverge. Implement damped-Gauss-Newton with backtracking line search. Use parameters s=1, \\alpha=\\beta=0.5 s=1, \\alpha=\\beta=0.5 . (You may need to force the line search to stop after a constant number of iterations.) Submit a plot that shows f(\\vx^{(k)}) f(\\vx^{(k)}) tracked for k = 1,..., 100 k = 1,..., 100 iterations, for all four solvers. Report also the values for \\bar \\alpha \\bar \\alpha in the constant step size cases. Write 2-3 sentences comparing each approach. Also, describe any deviations of your code to the standard approach. (e.g. any tweaks.)","title":"Homework 2"},{"location":"homework/hw2/hw2/#cpsc-406-homework-2-due-jan-27-6pm","text":"(Beck 3.1) Let \\mA\\in \\R^{m\\times n} \\mA\\in \\R^{m\\times n} , \\vb\\in \\R^n \\vb\\in \\R^n , \\mL \\in \\R^{p\\times n} \\mL \\in \\R^{p\\times n} and scalar \\lambda >0 \\lambda >0 . Consider the regularized least squares problem \\mathop{\\text{minimize}}_{\\vx\\in \\R^n} \\quad f(\\vx) := \\|\\mA \\vx-\\vb\\|_2^2 + \\lambda \\|\\mL\\vx\\|_2^2. \\mathop{\\text{minimize}}_{\\vx\\in \\R^n} \\quad f(\\vx) := \\|\\mA \\vx-\\vb\\|_2^2 + \\lambda \\|\\mL\\vx\\|_2^2. Show that this problem has a unique solution if and only if \\vnull(\\mA) \\cap \\vnull(\\mL) = \\{\\vzero\\} \\vnull(\\mA) \\cap \\vnull(\\mL) = \\{\\vzero\\} . Hint: Look up De Morgan's laws. Multiobjective problems. Often, in real world applications, we wish to accomplish multiple goals at once. For example, we may wish to buy the biggest house with the least amount of money, or buy the most profitable stocks which also have least risk. These objectives are often competing, and it is impossible to optimize one without negatively impacting the other. 2-norm regularization. Consider the 2-norm regularized least squares problem \\minimize{\\vx\\in \\R^n} \\quad \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\frac{\\gamma}{2}\\underbrace{\\|\\vx\\|_2^2}_{f_2(\\vx)} \\minimize{\\vx\\in \\R^n} \\quad \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\frac{\\gamma}{2}\\underbrace{\\|\\vx\\|_2^2}_{f_2(\\vx)} Show that at optimality, \\|\\vx\\|_2^2 = \\sum_{i=1}^n \\frac{1}{(d_i+\\gamma)^2} g_i^2 \\|\\vx\\|_2^2 = \\sum_{i=1}^n \\frac{1}{(d_i+\\gamma)^2} g_i^2 where \\mA\\trans\\mA = \\mQ\\mD\\mQ\\trans \\mA\\trans\\mA = \\mQ\\mD\\mQ\\trans the eigenvalue decomposition and \\vg = \\mQ\\trans\\mA\\trans\\vb \\vg = \\mQ\\trans\\mA\\trans\\vb . Roughly sketch what the Pareto-Frontier looks like here, paying attention to the limiting behavior ( \\gamma\\to 0 \\gamma\\to 0 and \\gamma\\to +\\infty \\gamma\\to +\\infty ). Sparsity. As an example, let us consider the problem of sparse recovery, where we wish to find the sparsest solution \\vx \\vx such that \\mA\\vx \\approx \\vb \\mA\\vx \\approx \\vb . Here, we motivate sparsity by minimizing the 1-norm of \\vx \\vx , e.g. f_1(\\vx) = \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\qquad f_2(\\vx) = \\|\\vx\\|_1 f_1(\\vx) = \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\qquad f_2(\\vx) = \\|\\vx\\|_1 and the goal is to make both f_1(\\vx) f_1(\\vx) and f_2(\\vx) f_2(\\vx) small. Download data \\mA\\in \\R^{m\\times n} \\mA\\in \\R^{m\\times n} ( jld , csv ) and \\vb\\in \\R^m \\vb\\in \\R^m ( jld , csv ). Using Convex.jl or CVX package solve the following problem for \\gamma = 1 \\gamma = 1 . \\minimize{x\\in \\R^n} \\; \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\gamma \\underbrace{\\|\\vx\\|_1}_{f_2(\\vx)} \\minimize{x\\in \\R^n} \\; \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\gamma \\underbrace{\\|\\vx\\|_1}_{f_2(\\vx)} Report the accuracy ( f_1(\\vx^*) f_1(\\vx^*) value) and sparsity metric ( f_2(\\vx^*) f_2(\\vx^*) value). To show the effect of the \"sparsity promoter\" term ( f_2(\\vx) f_2(\\vx) ), repeat the above exercise for \\gamma = 0.01 \\gamma = 0.01 and \\gamma = 10 \\gamma = 10 . Plot both solutions on top of the original signal \\vx_0 \\vx_0 ( jld , csv ). Which solution gives better signal recovery? Repeat the above exercise for 100 different values of \\gamma \\gamma , generated from exp10.(range(-3, stop=3, length=100)) . Plot the cost f_1(\\vx) f_1(\\vx) on the x x -axis and f_2(\\vx) f_2(\\vx) on the y y -axis. This is the Pareto frontier . For any cost value pair to the upper right of this curve, at least one cost can be made smaller at no expense to the other. For any cost value pair to the lower left of the curve, the cost values are unattainable. The Pareto frontier represents the \"best case scenario\" set of points. We now consider another example where we motivate smoothness as well as accuracy. Download data \\mA\\in \\R^{m\\times n} \\mA\\in \\R^{m\\times n} ( jld , csv ) and \\vb\\in \\R^m \\vb\\in \\R^m ( jld , csv ). Again, using Convex.jl or CVX package, solve the following problem for \\gamma = 1 \\gamma = 1 . \\minimize{\\vx\\in \\R^n} \\; \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\gamma \\underbrace{\\|\\mD\\vx\\|_1}_{f_2(\\vx)} \\minimize{\\vx\\in \\R^n} \\; \\underbrace{\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2}_{f_1(\\vx)} + \\gamma \\underbrace{\\|\\mD\\vx\\|_1}_{f_2(\\vx)} where \\mD = \\bmat 1 & -1 & 0 &\\cdots &0&0 \\\\ 0 &1 & -1 &\\cdots &0&0 \\\\ \\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & 1 & -1 \\emat \\in \\R^{n-1\\times n} \\mD = \\bmat 1 & -1 & 0 &\\cdots &0&0 \\\\ 0 &1 & -1 &\\cdots &0&0 \\\\ \\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & 1 & -1 \\emat \\in \\R^{n-1\\times n} Minimize this objective for \\gamma = 0.01 \\gamma = 0.01 and \\gamma = 10 \\gamma = 10 . Plot both solutions on top of the original signal \\vx_0 \\vx_0 ( jld , csv ). Which solutio)n gives better signal recovery? Repeat the above exercise for 100 different values of \\gamma \\gamma , generated from exp10.(range(-3, stop=3, length=100)) . Plot the Pareto frontier. Non-linear least squares (adapted from Beck 4.6) The source localization problem consists of minimizing \\minimize{\\vx\\in \\R^n} \\left\\{f(\\vx) := \\sum_{i=1}^m \\left(\\|\\vx-\\vc_i\\|_2^2 - d_i^2\\right)^2\\right\\} \\minimize{\\vx\\in \\R^n} \\left\\{f(\\vx) := \\sum_{i=1}^m \\left(\\|\\vx-\\vc_i\\|_2^2 - d_i^2\\right)^2\\right\\} for vectors \\vc_1,...,\\vc_m\\in \\R^n \\vc_1,...,\\vc_m\\in \\R^n and scalars d_1,...,d_m d_1,...,d_m . This is of course a nonlinear least squares problem, and thus the Gauss-Newton method can be employed to solve it. We will assume that n = 2 n = 2 . Compute the gradient of f(\\vx) f(\\vx) at a given point \\vx \\vx . Rewrite the problem as \\minimize{\\vx\\in \\R^n} \\quad \\|r(\\vx)\\|_2^2 \\minimize{\\vx\\in \\R^n} \\quad \\|r(\\vx)\\|_2^2 that is, identify r(\\vx) : \\R^n \\to \\R^m r(\\vx) : \\R^n \\to \\R^m . Compute also the Jacobian of r(\\vx) r(\\vx) J(\\vx) = \\bmat \\nabla r_1(\\vx)\\trans \\\\ \\vdots \\\\ \\nabla r_m(\\vx)\\trans\\emat. J(\\vx) = \\bmat \\nabla r_1(\\vx)\\trans \\\\ \\vdots \\\\ \\nabla r_m(\\vx)\\trans\\emat. Show that as long as all the points \\vc_1,...,\\vc_m \\vc_1,...,\\vc_m do not reside on the same line in the plane, and at each iterate \\vx^{(k)} \\neq \\vc_i \\vx^{(k)} \\neq \\vc_i for any i = 1,...,m i = 1,...,m , the method is well-defined, meaning that the linear least squares problem solved at each iteration has a unique solution. We will now implement some numerical methods on this problem. Download data \\mC\\in \\R^{2\\times 5} \\mC\\in \\R^{2\\times 5} ( jld , csv ), \\vd\\in \\R^5 \\vd\\in \\R^5 ( jld , csv ), and \\vx \\in \\R^2 \\vx \\in \\R^2 ( jld , csv ). The columns of the 2 \\times 5 2 \\times 5 matrix \\mC \\mC are the locations of the five sensors, \\vx \\vx is the \"true\" location of the source, and \\vd \\vd is the vector of noisy measurements between the source and the sensors. In all cases, initialize with \\vx^{(0)} = (1000, -500)\\trans \\vx^{(0)} = (1000, -500)\\trans . The gradient descent method operates via the iteration scheme \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)} \\nabla f(\\vx^{(k)}) \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)} \\nabla f(\\vx^{(k)}) for a choice of step size \\alpha^{(k)} \\alpha^{(k)} . Implement the gradient descent method for a constant step size \\alpha^{(k)} = \\bar \\alpha \\alpha^{(k)} = \\bar \\alpha . Try several values until you find the largest value of \\bar \\alpha \\bar \\alpha such that the method does not diverge. Hint: try several orders of magnitude from 10^{-10} 10^{-10} to 1 1 . Implement gradient descent with backtracking line search. Use parameters s=1, \\alpha=\\beta=0.5 s=1, \\alpha=\\beta=0.5 . (Refer also to page 51 in Beck .) Implement damped Gauss-Newton to minimize f(\\vx) f(\\vx) . Use the largest constant step size \\bar \\alpha \\leq 1 \\bar \\alpha \\leq 1 such that the method does not diverge. Implement damped-Gauss-Newton with backtracking line search. Use parameters s=1, \\alpha=\\beta=0.5 s=1, \\alpha=\\beta=0.5 . (You may need to force the line search to stop after a constant number of iterations.) Submit a plot that shows f(\\vx^{(k)}) f(\\vx^{(k)}) tracked for k = 1,..., 100 k = 1,..., 100 iterations, for all four solvers. Report also the values for \\bar \\alpha \\bar \\alpha in the constant step size cases. Write 2-3 sentences comparing each approach. Also, describe any deviations of your code to the standard approach. (e.g. any tweaks.)","title":"CPSC 406: Homework 2 (Due Jan  27, 6pm)"},{"location":"notes/Least_squares/","text":"Least Squares \u00b6 In this lecture, we will cover least squares for data fitting, linear systems, properties of least squares and QR factorization. Least squares for data fitting \u00b6 Consider the problem of fitting a line to observations y_i y_i gven input z_i z_i for i = 1,\\dots, n i = 1,\\dots, n . In the figure above, the data points seem to follow a linear trend. One way to find the parameters c,s \\in \\R c,s \\in \\R of the linear model f(z) = s\\cdot z + c f(z) = s\\cdot z + c that coresponds to a line of best fit is to minimize the following squared distance subject to a linear constraint: \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} The above minimization program can be reformulated as a linear least squares problem: \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, where \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. Let \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} The gradient and hessian of \\func{f}(\\vx) \\func{f}(\\vx) are: \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, respectively. The Hessian is positive semidefinite for every \\vx \\vx (and is positive definite if \\mA \\mA has full row rank). This implies that the function \\func{f}(\\vx) \\func{f}(\\vx) is convex. Additionally, \\vx = \\vx^{*} \\vx = \\vx^{*} is a critical point if \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} Since \\func{f}(\\vx) \\func{f}(\\vx) is convex, \\vx^{*} \\vx^{*} is a global minimizer. Equation \\eqref{least_square_normal_eqn} is called the normal equations of the least squares problem \\eqref{least_squares_problem}. Solving the normal equations, we get the following line of best fit. Linear systems \u00b6 Consider the problem of solving a linear system of equations. For \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in\\R^{m} \\vb\\in\\R^{m} , the linear system of equations \\mA\\vx = \\vb \\mA\\vx = \\vb is: overdetermined if m>n m>n , underdetermined if m< n m< n , or square if m = n m = n . A linear system can have exactly one solution, many solutions, or no solutions: In general, a linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has a solution if \\vb \\in \\text{range} (\\mA) \\vb \\in \\text{range} (\\mA) . Properties of linear least squares \u00b6 Recall that the minimizer \\vx^* \\vx^* to the linear least squares poblem satisfies the normal equations: \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb with the residual \\vr^* = \\mA\\vx^* -\\vb, \\vr^* = \\mA\\vx^* -\\vb, satisfying \\mA^\\intercal\\vr^* = \\vzero. \\mA^\\intercal\\vr^* = \\vzero. Here, \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The minimzer of the linear least squares problem is unique if \\mA^\\intercal\\mA \\mA^\\intercal\\mA is invertible. However, the vector in the range of \\mA \\mA closest to \\vb \\vb is unique, i.e. \\vb^* = \\mA\\vx* \\vb^* = \\mA\\vx* is unique. Recall that range space of \\mA \\mA and the null space of \\mA^\\intercal \\mA^\\intercal is: \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} By fundamental theorem of linear algebra, we have $$ \\begin{equation}\\label{least_squares_FTLA} \\set{R}(\\mA) \\oplus \\set{N}(\\mA^\\intercal) = \\R^m. \\end{equation} $$ Thus, for all \\vx \\in \\R^m \\vx \\in \\R^m , we have \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) with \\vu \\vu and \\vv \\vv uniquely determined. This is illustrated in the figure below: Here, \\vx_{LS} \\vx_{LS} is the least squares solution, \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} , with \\va_i \\in \\R^m \\va_i \\in \\R^m for all i i . Comparing with \\eqref{least_squares_FTLA}, we get \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\exa{1} \\exa{1} What is the least-squares solution \\vx^* \\vx^* for the problem \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, where \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\text{Solution:} \\text{Solution:} First setup the normal equations: \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb Solving the normal equations, we get \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb So, the least squares solution \\vx^* \\vx^* is the mean value of the elements in \\vb \\vb .","title":"Least squares"},{"location":"notes/Least_squares/#least-squares","text":"In this lecture, we will cover least squares for data fitting, linear systems, properties of least squares and QR factorization.","title":"Least Squares"},{"location":"notes/Least_squares/#least-squares-for-data-fitting","text":"Consider the problem of fitting a line to observations y_i y_i gven input z_i z_i for i = 1,\\dots, n i = 1,\\dots, n . In the figure above, the data points seem to follow a linear trend. One way to find the parameters c,s \\in \\R c,s \\in \\R of the linear model f(z) = s\\cdot z + c f(z) = s\\cdot z + c that coresponds to a line of best fit is to minimize the following squared distance subject to a linear constraint: \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} The above minimization program can be reformulated as a linear least squares problem: \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, where \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. Let \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} The gradient and hessian of \\func{f}(\\vx) \\func{f}(\\vx) are: \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, respectively. The Hessian is positive semidefinite for every \\vx \\vx (and is positive definite if \\mA \\mA has full row rank). This implies that the function \\func{f}(\\vx) \\func{f}(\\vx) is convex. Additionally, \\vx = \\vx^{*} \\vx = \\vx^{*} is a critical point if \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} Since \\func{f}(\\vx) \\func{f}(\\vx) is convex, \\vx^{*} \\vx^{*} is a global minimizer. Equation \\eqref{least_square_normal_eqn} is called the normal equations of the least squares problem \\eqref{least_squares_problem}. Solving the normal equations, we get the following line of best fit.","title":"Least squares for data fitting"},{"location":"notes/Least_squares/#linear-systems","text":"Consider the problem of solving a linear system of equations. For \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in\\R^{m} \\vb\\in\\R^{m} , the linear system of equations \\mA\\vx = \\vb \\mA\\vx = \\vb is: overdetermined if m>n m>n , underdetermined if m< n m< n , or square if m = n m = n . A linear system can have exactly one solution, many solutions, or no solutions: In general, a linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has a solution if \\vb \\in \\text{range} (\\mA) \\vb \\in \\text{range} (\\mA) .","title":"Linear systems"},{"location":"notes/Least_squares/#properties-of-linear-least-squares","text":"Recall that the minimizer \\vx^* \\vx^* to the linear least squares poblem satisfies the normal equations: \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb with the residual \\vr^* = \\mA\\vx^* -\\vb, \\vr^* = \\mA\\vx^* -\\vb, satisfying \\mA^\\intercal\\vr^* = \\vzero. \\mA^\\intercal\\vr^* = \\vzero. Here, \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The minimzer of the linear least squares problem is unique if \\mA^\\intercal\\mA \\mA^\\intercal\\mA is invertible. However, the vector in the range of \\mA \\mA closest to \\vb \\vb is unique, i.e. \\vb^* = \\mA\\vx* \\vb^* = \\mA\\vx* is unique. Recall that range space of \\mA \\mA and the null space of \\mA^\\intercal \\mA^\\intercal is: \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} By fundamental theorem of linear algebra, we have $$ \\begin{equation}\\label{least_squares_FTLA} \\set{R}(\\mA) \\oplus \\set{N}(\\mA^\\intercal) = \\R^m. \\end{equation} $$ Thus, for all \\vx \\in \\R^m \\vx \\in \\R^m , we have \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) with \\vu \\vu and \\vv \\vv uniquely determined. This is illustrated in the figure below: Here, \\vx_{LS} \\vx_{LS} is the least squares solution, \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} , with \\va_i \\in \\R^m \\va_i \\in \\R^m for all i i . Comparing with \\eqref{least_squares_FTLA}, we get \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\exa{1} \\exa{1} What is the least-squares solution \\vx^* \\vx^* for the problem \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, where \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\text{Solution:} \\text{Solution:} First setup the normal equations: \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb Solving the normal equations, we get \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb So, the least squares solution \\vx^* \\vx^* is the mean value of the elements in \\vb \\vb .","title":"Properties of linear least squares"},{"location":"notes/Non-linear_LS/","text":"Non-linear least squares \u00b6 The non-linear least squares problem is \\begin{equation}\\label{Non-linearleastsquares_prob} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2, \\end{equation} \\begin{equation}\\label{Non-linearleastsquares_prob} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2, \\end{equation} where r:\\R^n\u2192\\R^m r:\\R^n\u2192\\R^m is the residual vector. The i i th component of residual vector is r_{i}(\\vx):\\R^n\u2192\\R r_{i}(\\vx):\\R^n\u2192\\R . The non-linear least squares problem reduces to the linear least squares problem if r r is affine, i.e. r(\\vx) = \\mA\\vx-\\vb r(\\vx) = \\mA\\vx-\\vb . Example: Position estimation from ranges Let \\vx \\in \\R^2 \\vx \\in \\R^2 be an unknown vector. Fix m m beacon positions \\vb_{i} \\in \\R^2,\\ i = 1,\\dots,m \\vb_{i} \\in \\R^2,\\ i = 1,\\dots,m . Suppose we have noisy measurements \\vrho \\in \\R^m \\vrho \\in \\R^m of 2 2 -norm distance between a becon \\vb_{i} \\vb_{i} and the unknown signal \\vx \\vx , i.e. $$ \u03c1_{i} = |\\vx- \\vb|_2 + \u03bd_i \\quad \\text{for } i=1,\\dots,m. $$ Here, \\vnu \\in \\R^m \\vnu \\in \\R^m is noise/measurement error vector. The position estimation from ranges problem is to estimate \\vx \\vx given \\vrho \\vrho and \\vb_i, \\ i = 1,\\dots, m \\vb_i, \\ i = 1,\\dots, m . A natural approach to solve this problem is by finding \\vx \\vx that minimizes \\sum_{i=1}^m(\u03c1_{i} - \\|\\vx- \\vb\\|_2)^2 \\sum_{i=1}^m(\u03c1_{i} - \\|\\vx- \\vb\\|_2)^2 . Define r_i(\\vx) := \u03c1_{i} - \\|\\vx- \\vb\\|_2 r_i(\\vx) := \u03c1_{i} - \\|\\vx- \\vb\\|_2 . Then we can estimmate \\vx \\vx by solving the non-linear least squares problem \\min_{\\vx\\in \\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2. \\min_{\\vx\\in \\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2. In contrast to linear least squares program, the non-linear least squares program generally contain both global and local minimizers. We can will use the following approach to find a minimizer of NLLS. Gauss-Newton method for NLLS Given starting guess \\vx^{(0)} \\vx^{(0)} Repeat until covergence: linearize r(x) r(x) near current guess \\bar{\\vx} = \\vx^{k} \\bar{\\vx} = \\vx^{k} solve a linear least squares problem to get the next guess \\vx^{k+1} \\vx^{k+1} , Linearization of residual \u00b6 We can solve non-linear least squares problem \\eqref{Non-linearleastsquares_prob} by solving a sequence of linear least squares problem. These linear least squares subproblem results from linearization of r(\\vx) r(\\vx) at current estimate of the ground truth \\vx \\vx . The linear approximation of r(\\vx) r(\\vx) at a point \\bar{\\vx} \\in \\R^n \\bar{\\vx} \\in \\R^n is r(\\vx) = \\bmat r_1(\\vx)\\\\\\vdots\\\\ r_n(\\vx)\\emat \\approx \\bmat r_1(\\bar{\\vx}) +\\nabla r_1(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\\\ \\vdots \\\\ r_m(\\bar{\\vx}) +\\nabla r_m(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\emat = A(\\bar{\\vx}) \\vx -b(\\bar{\\vx}), r(\\vx) = \\bmat r_1(\\vx)\\\\\\vdots\\\\ r_n(\\vx)\\emat \\approx \\bmat r_1(\\bar{\\vx}) +\\nabla r_1(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\\\ \\vdots \\\\ r_m(\\bar{\\vx}) +\\nabla r_m(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\emat = A(\\bar{\\vx}) \\vx -b(\\bar{\\vx}), where A(\\bar{\\vx})\\in\\R^{m\\times n} A(\\bar{\\vx})\\in\\R^{m\\times n} is the Jacobian of the mappring r(x) r(x) at \\bar{\\vx} \\bar{\\vx} and b(\\bar{\\vx}) = A(\\bar{\\vx})\\vx - r(\\bar{\\vx}) \\in \\R^m b(\\bar{\\vx}) = A(\\bar{\\vx})\\vx - r(\\bar{\\vx}) \\in \\R^m . The Jacobian of r(x) r(x) at \\bar{\\vx} \\bar{\\vx} is A(\\bar{\\vx}) = \\bmat \\nabla r_1(\\bar{\\vx})\\trans\\\\ \\vdots \\\\ \\nabla r_m(\\bar{\\vx})\\trans\\emat. A(\\bar{\\vx}) = \\bmat \\nabla r_1(\\bar{\\vx})\\trans\\\\ \\vdots \\\\ \\nabla r_m(\\bar{\\vx})\\trans\\emat. We get the following minimization program after replacing r(\\vx) r(\\vx) with its linear approximation at \\vx^{(k)} \\vx^{(k)} : \\vx^{(k+1)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|A(\\vx^{(k)})\\vx - b(\\vx^{(k)})\\|_2^2. \\vx^{(k+1)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|A(\\vx^{(k)})\\vx - b(\\vx^{(k)})\\|_2^2. Starting at a current estimate \\vx^{(k)} \\vx^{(k)} , we can determine the \\vx^{(k+1)} \\vx^{(k+1)} by solving the above linear least squares program. Dampening \u00b6 For ease of exposition, let \\bar{\\mA} = A(\\vx^{(k)}), \\quad \\bar{\\vb} = b(\\vx^{(k)}), \\text{ and } \\bar{\\vr} = r(\\vx^{(k)}). \\bar{\\mA} = A(\\vx^{(k)}), \\quad \\bar{\\vb} = b(\\vx^{(k)}), \\text{ and } \\bar{\\vr} = r(\\vx^{(k)}). We assume that \\bar{\\mA} \\bar{\\mA} is full rank. Consider \\begin{align*} \\vx^{(k+1)} = &\\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vb}\\|_2^2\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vb}\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans(\\bar{\\mA}\\vx^{(k)} - \\bar{\\vr})\\\\ = &\\vx^{(k)} - (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} \\end{align*} \\begin{align*} \\vx^{(k+1)} = &\\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vb}\\|_2^2\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vb}\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans(\\bar{\\mA}\\vx^{(k)} - \\bar{\\vr})\\\\ = &\\vx^{(k)} - (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} \\end{align*} Here, \\vx^{(k+1)} \\vx^{(k+1)} is the k+1 k+1 Gauss-Newton estimate. Note that (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} solves \\min_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2 \\min_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2 . Let \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2. \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2. The dampened Gauss-Newton step is \\vx^{(k+1)} = \\vx^{(k)} - \\alpha \\vz^{(k)}, \\vx^{(k+1)} = \\vx^{(k)} - \\alpha \\vz^{(k)}, where \\alpha \\in (0,1] \\alpha \\in (0,1] . Dampened Gauss-Newton method for NLLS Given starting guess \\vx^{(0)} \\vx^{(0)} Repeat until covergence: linearize r(x) r(x) near current guess \\bar{\\vx} = \\vx^{k} \\bar{\\vx} = \\vx^{k} : \\quad r(\\vx) \\approx r(\\bar{\\vx}) - A(\\bar{\\vx})(\\vx-\\bar{\\vx}) \\quad r(\\vx) \\approx r(\\bar{\\vx}) - A(\\bar{\\vx})(\\vx-\\bar{\\vx}) solve a linear least squares problem: \\quad \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n}\\|A(\\bar{\\vx})\\vx - r(\\bar{\\vx})\\|_2^2 \\quad \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n}\\|A(\\bar{\\vx})\\vx - r(\\bar{\\vx})\\|_2^2 take damped step: \\quad \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)}\\vz^{(k)}, \\quad 0<\\alpha^{(k)}\\leq 1 \\quad \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)}\\vz^{(k)}, \\quad 0<\\alpha^{(k)}\\leq 1 until converged","title":"Non-linear least squares"},{"location":"notes/Non-linear_LS/#non-linear-least-squares","text":"The non-linear least squares problem is \\begin{equation}\\label{Non-linearleastsquares_prob} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2, \\end{equation} \\begin{equation}\\label{Non-linearleastsquares_prob} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2, \\end{equation} where r:\\R^n\u2192\\R^m r:\\R^n\u2192\\R^m is the residual vector. The i i th component of residual vector is r_{i}(\\vx):\\R^n\u2192\\R r_{i}(\\vx):\\R^n\u2192\\R . The non-linear least squares problem reduces to the linear least squares problem if r r is affine, i.e. r(\\vx) = \\mA\\vx-\\vb r(\\vx) = \\mA\\vx-\\vb . Example: Position estimation from ranges Let \\vx \\in \\R^2 \\vx \\in \\R^2 be an unknown vector. Fix m m beacon positions \\vb_{i} \\in \\R^2,\\ i = 1,\\dots,m \\vb_{i} \\in \\R^2,\\ i = 1,\\dots,m . Suppose we have noisy measurements \\vrho \\in \\R^m \\vrho \\in \\R^m of 2 2 -norm distance between a becon \\vb_{i} \\vb_{i} and the unknown signal \\vx \\vx , i.e. $$ \u03c1_{i} = |\\vx- \\vb|_2 + \u03bd_i \\quad \\text{for } i=1,\\dots,m. $$ Here, \\vnu \\in \\R^m \\vnu \\in \\R^m is noise/measurement error vector. The position estimation from ranges problem is to estimate \\vx \\vx given \\vrho \\vrho and \\vb_i, \\ i = 1,\\dots, m \\vb_i, \\ i = 1,\\dots, m . A natural approach to solve this problem is by finding \\vx \\vx that minimizes \\sum_{i=1}^m(\u03c1_{i} - \\|\\vx- \\vb\\|_2)^2 \\sum_{i=1}^m(\u03c1_{i} - \\|\\vx- \\vb\\|_2)^2 . Define r_i(\\vx) := \u03c1_{i} - \\|\\vx- \\vb\\|_2 r_i(\\vx) := \u03c1_{i} - \\|\\vx- \\vb\\|_2 . Then we can estimmate \\vx \\vx by solving the non-linear least squares problem \\min_{\\vx\\in \\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2. \\min_{\\vx\\in \\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2. In contrast to linear least squares program, the non-linear least squares program generally contain both global and local minimizers. We can will use the following approach to find a minimizer of NLLS. Gauss-Newton method for NLLS Given starting guess \\vx^{(0)} \\vx^{(0)} Repeat until covergence: linearize r(x) r(x) near current guess \\bar{\\vx} = \\vx^{k} \\bar{\\vx} = \\vx^{k} solve a linear least squares problem to get the next guess \\vx^{k+1} \\vx^{k+1} ,","title":"Non-linear least squares"},{"location":"notes/Non-linear_LS/#linearization-of-residual","text":"We can solve non-linear least squares problem \\eqref{Non-linearleastsquares_prob} by solving a sequence of linear least squares problem. These linear least squares subproblem results from linearization of r(\\vx) r(\\vx) at current estimate of the ground truth \\vx \\vx . The linear approximation of r(\\vx) r(\\vx) at a point \\bar{\\vx} \\in \\R^n \\bar{\\vx} \\in \\R^n is r(\\vx) = \\bmat r_1(\\vx)\\\\\\vdots\\\\ r_n(\\vx)\\emat \\approx \\bmat r_1(\\bar{\\vx}) +\\nabla r_1(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\\\ \\vdots \\\\ r_m(\\bar{\\vx}) +\\nabla r_m(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\emat = A(\\bar{\\vx}) \\vx -b(\\bar{\\vx}), r(\\vx) = \\bmat r_1(\\vx)\\\\\\vdots\\\\ r_n(\\vx)\\emat \\approx \\bmat r_1(\\bar{\\vx}) +\\nabla r_1(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\\\ \\vdots \\\\ r_m(\\bar{\\vx}) +\\nabla r_m(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\emat = A(\\bar{\\vx}) \\vx -b(\\bar{\\vx}), where A(\\bar{\\vx})\\in\\R^{m\\times n} A(\\bar{\\vx})\\in\\R^{m\\times n} is the Jacobian of the mappring r(x) r(x) at \\bar{\\vx} \\bar{\\vx} and b(\\bar{\\vx}) = A(\\bar{\\vx})\\vx - r(\\bar{\\vx}) \\in \\R^m b(\\bar{\\vx}) = A(\\bar{\\vx})\\vx - r(\\bar{\\vx}) \\in \\R^m . The Jacobian of r(x) r(x) at \\bar{\\vx} \\bar{\\vx} is A(\\bar{\\vx}) = \\bmat \\nabla r_1(\\bar{\\vx})\\trans\\\\ \\vdots \\\\ \\nabla r_m(\\bar{\\vx})\\trans\\emat. A(\\bar{\\vx}) = \\bmat \\nabla r_1(\\bar{\\vx})\\trans\\\\ \\vdots \\\\ \\nabla r_m(\\bar{\\vx})\\trans\\emat. We get the following minimization program after replacing r(\\vx) r(\\vx) with its linear approximation at \\vx^{(k)} \\vx^{(k)} : \\vx^{(k+1)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|A(\\vx^{(k)})\\vx - b(\\vx^{(k)})\\|_2^2. \\vx^{(k+1)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|A(\\vx^{(k)})\\vx - b(\\vx^{(k)})\\|_2^2. Starting at a current estimate \\vx^{(k)} \\vx^{(k)} , we can determine the \\vx^{(k+1)} \\vx^{(k+1)} by solving the above linear least squares program.","title":"Linearization of residual"},{"location":"notes/Non-linear_LS/#dampening","text":"For ease of exposition, let \\bar{\\mA} = A(\\vx^{(k)}), \\quad \\bar{\\vb} = b(\\vx^{(k)}), \\text{ and } \\bar{\\vr} = r(\\vx^{(k)}). \\bar{\\mA} = A(\\vx^{(k)}), \\quad \\bar{\\vb} = b(\\vx^{(k)}), \\text{ and } \\bar{\\vr} = r(\\vx^{(k)}). We assume that \\bar{\\mA} \\bar{\\mA} is full rank. Consider \\begin{align*} \\vx^{(k+1)} = &\\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vb}\\|_2^2\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vb}\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans(\\bar{\\mA}\\vx^{(k)} - \\bar{\\vr})\\\\ = &\\vx^{(k)} - (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} \\end{align*} \\begin{align*} \\vx^{(k+1)} = &\\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vb}\\|_2^2\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vb}\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans(\\bar{\\mA}\\vx^{(k)} - \\bar{\\vr})\\\\ = &\\vx^{(k)} - (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} \\end{align*} Here, \\vx^{(k+1)} \\vx^{(k+1)} is the k+1 k+1 Gauss-Newton estimate. Note that (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} solves \\min_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2 \\min_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2 . Let \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2. \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2. The dampened Gauss-Newton step is \\vx^{(k+1)} = \\vx^{(k)} - \\alpha \\vz^{(k)}, \\vx^{(k+1)} = \\vx^{(k)} - \\alpha \\vz^{(k)}, where \\alpha \\in (0,1] \\alpha \\in (0,1] . Dampened Gauss-Newton method for NLLS Given starting guess \\vx^{(0)} \\vx^{(0)} Repeat until covergence: linearize r(x) r(x) near current guess \\bar{\\vx} = \\vx^{k} \\bar{\\vx} = \\vx^{k} : \\quad r(\\vx) \\approx r(\\bar{\\vx}) - A(\\bar{\\vx})(\\vx-\\bar{\\vx}) \\quad r(\\vx) \\approx r(\\bar{\\vx}) - A(\\bar{\\vx})(\\vx-\\bar{\\vx}) solve a linear least squares problem: \\quad \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n}\\|A(\\bar{\\vx})\\vx - r(\\bar{\\vx})\\|_2^2 \\quad \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n}\\|A(\\bar{\\vx})\\vx - r(\\bar{\\vx})\\|_2^2 take damped step: \\quad \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)}\\vz^{(k)}, \\quad 0<\\alpha^{(k)}\\leq 1 \\quad \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)}\\vz^{(k)}, \\quad 0<\\alpha^{(k)}\\leq 1 until converged","title":"Dampening"},{"location":"notes/QR_factorization/","text":"QR factorization \u00b6 Orthogonal and orthonormal vectors \u00b6 Let \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n be any two vectors. By cosine identity, we have \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) where, \\theta \\theta is the angle between \\vx \\vx and \\vy \\vy . So, \\vx \\vx and \\vy \\vy are orthogonal ( \\theta = 0 \\theta = 0 ), if \\vx\\trans\\vy = 0 \\vx\\trans\\vy = 0 . Furthermore, we say \\vx \\vx and \\vy \\vy are orthonomal if \\vx \\vx and \\vy \\vy have unit 2-norm and are orthogonal, i.e. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1. Orthogonal matrices \u00b6 A matrix \\mQ \\mQ is orthogonal if it is square and its columns are all pairwise orthogonal. For an orthogonal matrix \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat , we have \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. Since, a matrix \\mB \\mB is the inverse of a matrix \\mA \\mA if \\mB\\mA =\\mA\\mB = \\mI \\mB\\mA =\\mA\\mB = \\mI , the inverse of an orthognal matrix is its transpose, i.e. \\mQ^{-1} = \\mQ\\trans. \\mQ^{-1} = \\mQ\\trans. Orthogonal matrices have many good properties. One such property is that inner products are invariant under orthogonal transfromations. So, for any vectors \\vx,\\ \\vy \\vx,\\ \\vy , we have (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. This also implies that 2-norm is invariant to orthogonal transformations as \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 . Another property of othogonal matrices is that their determinant is either 1 1 or -1 -1 . This can be observed from the fact that for a matrix \\mA \\mA and \\mB \\mB , we have \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) and \\det(\\mA)= \\det(\\mA\\trans) \\det(\\mA)= \\det(\\mA\\trans) . So, from \\mQ\\trans\\mQ = \\mI \\mQ\\trans\\mQ = \\mI , we get \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11. \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11. QR factorization \u00b6 Let \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . A factorization of \\mA \\mA with \\mA = \\mQ \\mR \\mA = \\mQ \\mR where \\mQ\\in\\R^{m\\times m} \\mQ\\in\\R^{m\\times m} is an orthogonal matrix and \\mR \\in \\R^{m\\times n} \\mR \\in \\R^{m\\times n} is an upper triangular matrix is called a QR QR factorization of \\mA \\mA . In the case with m\\geq n m\\geq n and \\rank(\\mA) = k \\leq n \\rank(\\mA) = k \\leq n , the QR QR facorization will have the following shape: In the above figure, \\mQ \\mQ is an orthogonal matrix. \\mR \\mR is a upper triangular matrix, i.e. R_{ij}=0 R_{ij}=0 whenever i>j i>j . \\hat{\\mQ} \\hat{\\mQ} spans the range of \\mA \\mA . \\bar{\\mQ} \\bar{\\mQ} spans the nullspace of \\mA\\trans \\mA\\trans . In Julia, we can compute the QR decompisition of a matrix using: m = 4 n = 3 A = randn ( m , n ) F = qr ( A ) Let \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat . We can express the columns of \\mA \\mA as \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} So, we can compactly write the matrix \\mA \\mA as \\mA = \\hat{\\mQ}\\hat{\\mR} \\mA = \\hat{\\mQ}\\hat{\\mR} . This is the reduced (thin or economode) QR factorization of \\mA \\mA . In the case when m\\geq n m\\geq n and \\mA \\mA is full rank, we get following figure: Solving least squares via QR \u00b6 The QR QR factorization can be used to solve the least squares problem \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in \\R^m \\vb\\in \\R^m . We consider the case where m\\geq n m\\geq n , but QR factorization can be used to solve the other case as well. Consider \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} So, minimizing \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 will minimize \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 . In the case \\mA \\mA is full rank, we get an invertible \\hat{\\mR} \\hat{\\mR} and the least squares solution which satisfies \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} will be unique. In the case when \\mA \\mA is not full rank, there will be a infinitely many solutions to the least squares problem. For both cases, we can find a least squares solution by solving \\eqref{QR_leastsquares} via back substitution. The figure below shows the geometric prespective of using a QR factorization to solve the least squares problem. For every \\vb \\in \\R^m \\vb \\in \\R^m , \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb is the orthogonal projection of \\vb \\vb onto the \\range(\\hat{\\mQ}) = \\range(\\mA) \\range(\\hat{\\mQ}) = \\range(\\mA) . The least squares solution finds a point \\vx \\vx such that \\mA\\vx \\mA\\vx is equal tothe orthogonal projection of \\vb \\vb onto the \\range(\\mA) \\range(\\mA) . So, we get \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb , which simplifies to \\eqref{QR_leastsquares}.","title":"QR factorization"},{"location":"notes/QR_factorization/#qr-factorization","text":"","title":"QR factorization"},{"location":"notes/QR_factorization/#orthogonal-and-orthonormal-vectors","text":"Let \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n be any two vectors. By cosine identity, we have \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) where, \\theta \\theta is the angle between \\vx \\vx and \\vy \\vy . So, \\vx \\vx and \\vy \\vy are orthogonal ( \\theta = 0 \\theta = 0 ), if \\vx\\trans\\vy = 0 \\vx\\trans\\vy = 0 . Furthermore, we say \\vx \\vx and \\vy \\vy are orthonomal if \\vx \\vx and \\vy \\vy have unit 2-norm and are orthogonal, i.e. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1.","title":"Orthogonal and orthonormal vectors"},{"location":"notes/QR_factorization/#orthogonal-matrices","text":"A matrix \\mQ \\mQ is orthogonal if it is square and its columns are all pairwise orthogonal. For an orthogonal matrix \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat , we have \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. Since, a matrix \\mB \\mB is the inverse of a matrix \\mA \\mA if \\mB\\mA =\\mA\\mB = \\mI \\mB\\mA =\\mA\\mB = \\mI , the inverse of an orthognal matrix is its transpose, i.e. \\mQ^{-1} = \\mQ\\trans. \\mQ^{-1} = \\mQ\\trans. Orthogonal matrices have many good properties. One such property is that inner products are invariant under orthogonal transfromations. So, for any vectors \\vx,\\ \\vy \\vx,\\ \\vy , we have (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. This also implies that 2-norm is invariant to orthogonal transformations as \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 . Another property of othogonal matrices is that their determinant is either 1 1 or -1 -1 . This can be observed from the fact that for a matrix \\mA \\mA and \\mB \\mB , we have \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) and \\det(\\mA)= \\det(\\mA\\trans) \\det(\\mA)= \\det(\\mA\\trans) . So, from \\mQ\\trans\\mQ = \\mI \\mQ\\trans\\mQ = \\mI , we get \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11. \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11.","title":"Orthogonal matrices"},{"location":"notes/QR_factorization/#qr-factorization_1","text":"Let \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . A factorization of \\mA \\mA with \\mA = \\mQ \\mR \\mA = \\mQ \\mR where \\mQ\\in\\R^{m\\times m} \\mQ\\in\\R^{m\\times m} is an orthogonal matrix and \\mR \\in \\R^{m\\times n} \\mR \\in \\R^{m\\times n} is an upper triangular matrix is called a QR QR factorization of \\mA \\mA . In the case with m\\geq n m\\geq n and \\rank(\\mA) = k \\leq n \\rank(\\mA) = k \\leq n , the QR QR facorization will have the following shape: In the above figure, \\mQ \\mQ is an orthogonal matrix. \\mR \\mR is a upper triangular matrix, i.e. R_{ij}=0 R_{ij}=0 whenever i>j i>j . \\hat{\\mQ} \\hat{\\mQ} spans the range of \\mA \\mA . \\bar{\\mQ} \\bar{\\mQ} spans the nullspace of \\mA\\trans \\mA\\trans . In Julia, we can compute the QR decompisition of a matrix using: m = 4 n = 3 A = randn ( m , n ) F = qr ( A ) Let \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat . We can express the columns of \\mA \\mA as \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} So, we can compactly write the matrix \\mA \\mA as \\mA = \\hat{\\mQ}\\hat{\\mR} \\mA = \\hat{\\mQ}\\hat{\\mR} . This is the reduced (thin or economode) QR factorization of \\mA \\mA . In the case when m\\geq n m\\geq n and \\mA \\mA is full rank, we get following figure:","title":"QR factorization"},{"location":"notes/QR_factorization/#solving-least-squares-via-qr","text":"The QR QR factorization can be used to solve the least squares problem \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in \\R^m \\vb\\in \\R^m . We consider the case where m\\geq n m\\geq n , but QR factorization can be used to solve the other case as well. Consider \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} So, minimizing \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 will minimize \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 . In the case \\mA \\mA is full rank, we get an invertible \\hat{\\mR} \\hat{\\mR} and the least squares solution which satisfies \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} will be unique. In the case when \\mA \\mA is not full rank, there will be a infinitely many solutions to the least squares problem. For both cases, we can find a least squares solution by solving \\eqref{QR_leastsquares} via back substitution. The figure below shows the geometric prespective of using a QR factorization to solve the least squares problem. For every \\vb \\in \\R^m \\vb \\in \\R^m , \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb is the orthogonal projection of \\vb \\vb onto the \\range(\\hat{\\mQ}) = \\range(\\mA) \\range(\\hat{\\mQ}) = \\range(\\mA) . The least squares solution finds a point \\vx \\vx such that \\mA\\vx \\mA\\vx is equal tothe orthogonal projection of \\vb \\vb onto the \\range(\\mA) \\range(\\mA) . So, we get \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb , which simplifies to \\eqref{QR_leastsquares}.","title":"Solving least squares via QR"},{"location":"notes/Regularized_LS/","text":"Regularized least squares \u00b6 In the least squares problem, we minimized 2-norm squared of the data misfit relative to a linear model. In contrast to the least squares formulation, many problem need to balance competing objectives. For example, consider the problem of finding \\vx_0 \\in \\R^n \\vx_0 \\in \\R^n from noisy linear measurements \\vb = \\mA\\vx_0 + \\vw_\\vb \\vb = \\mA\\vx_0 + \\vw_\\vb and \\vg = \\mF\\vx_0-\\vw_\\vg \\vg = \\mF\\vx_0-\\vw_\\vg . Here \\vw_\\vb \\vw_\\vb and \\vw_\\vg \\vw_\\vg are noise vectors. In order to solve this problem, we need to find a \\vx \\vx that makes \\func{f}_1(\\vx) = \\|\\mA\\vx - \\vb\\|_2^2 \\func{f}_1(\\vx) = \\|\\mA\\vx - \\vb\\|_2^2 small, and \\func{f}_2(\\vx) = \\|\\mB\\vx - \\vg\\|_2^2 \\func{f}_2(\\vx) = \\|\\mB\\vx - \\vg\\|_2^2 small. Generally, we can make \\func{f}_1(\\vx) \\func{f}_1(\\vx) or \\func{f}_2(\\vx) \\func{f}_2(\\vx) small, but not both. The figure below shows this relationship between \\func{f}_1(\\vx) \\func{f}_1(\\vx) and \\func{f}_2(\\vx) \\func{f}_2(\\vx) . In the figure, the points in the boundary of two regions are called the Pareto optimal solutions. In order to find these optimal solutions, we minimize the following weighted sum objective: \\begin{equation}\\label{Regularized_LS_weight} f_1(\\vx)+\\gamma f_2(\\vx) = \\|\\mA\\vx-\\vg\\|_2^2+\u03b3\\|\\mF\\vx-\\vg\\|_2^2. \\end{equation} \\begin{equation}\\label{Regularized_LS_weight} f_1(\\vx)+\\gamma f_2(\\vx) = \\|\\mA\\vx-\\vg\\|_2^2+\u03b3\\|\\mF\\vx-\\vg\\|_2^2. \\end{equation} The parameter \\gamma \\gamma is non-negative and defines relative weight between the objectives. For example, in the case of \\gamma =1 \\gamma =1 , the optimal point that minimizes \\eqref{Regularized_LS_weight} is the point \\vx \\vx on the optimal trade-off curve with f_1(\\vx) = f_2(\\vx) f_1(\\vx) = f_2(\\vx) . Note that for a fixed \\gamma \\gamma and \\alpha \\in \\R \\alpha \\in \\R , the set \\ell(\\gamma,\\alpha) = \\{(f_1(\\vx),f_2(\\vx)):f_1(\\vx) +\\gamma f_2(\\vx) = \\alpha, \\vx \\in \\R^n\\} \\ell(\\gamma,\\alpha) = \\{(f_1(\\vx),f_2(\\vx)):f_1(\\vx) +\\gamma f_2(\\vx) = \\alpha, \\vx \\in \\R^n\\} correspond to a line with slope of -\\gamma -\\gamma . Another way to visualize the optimal solution is to find the line that is tangent to the optimal trade-off cuve, see figure below. Example: Signal denoising \u00b6 Suppose we observe noisy measurements of a signal: \\vb = \\hat{\\vx} + \\vw, \\vb = \\hat{\\vx} + \\vw, where \\hat{\\vx}\\in\\R^n \\hat{\\vx}\\in\\R^n is the signal and \\vw \\in \\R^n \\vw \\in \\R^n is noise. A simple apporach to find \\hat{\\vx} \\hat{\\vx} is to solve: \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2. \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2. This minimization program doesnot enforce any structure on \\vx \\vx . However, if we have prior information that the signal is \"smooth\", then we might balance the least squares fit against the smoothness of the solution in the following way: \\begin{equation}\\label{Regularized_LS_identity} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2 +\\frac{\\gamma}{2} \\underbrace{\\sum_{i=1}^{n-1}(x_i - x_{i+})^2}_{f_2(\\vx)}. \\end{equation} \\begin{equation}\\label{Regularized_LS_identity} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2 +\\frac{\\gamma}{2} \\underbrace{\\sum_{i=1}^{n-1}(x_i - x_{i+})^2}_{f_2(\\vx)}. \\end{equation} Here, f_2(x) f_2(x) promotes smoothness. We can alternatively write the above minimization program in matrix notation. Define the finite differencem matrix \\mD = \\bmat 1 & -1 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & -1 & 0 & \\cdots & 0\\\\ & & \\ddots & \\ddots & & \\\\ 0 & \\cdots & 0 & 1 & -1 & 0\\\\ 0 & 0 & \\cdots & 0 & 1 & -1\\emat \\in \\R^{{n-1}\\times n} \\mD = \\bmat 1 & -1 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & -1 & 0 & \\cdots & 0\\\\ & & \\ddots & \\ddots & & \\\\ 0 & \\cdots & 0 & 1 & -1 & 0\\\\ 0 & 0 & \\cdots & 0 & 1 & -1\\emat \\in \\R^{{n-1}\\times n} So, we can rewrite f_2(\\vx) = \\sum_{i=1}^{n-1}(x_i - x_{i+})^2 = \\|\\mD\\vx\\|_2^2 f_2(\\vx) = \\sum_{i=1}^{n-1}(x_i - x_{i+})^2 = \\|\\mD\\vx\\|_2^2 . This allows for a reformulation of the weighted leas squares objective into a familiar least squares objective: \\|\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mI\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. \\|\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mI\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. So the solution to the weighted least squares minimization program \\eqref{Regularized_LS_identity} satisfies the normal equation \\hat{\\mA}\\hat{\\mA}^\\intercal\\vx = \\hat{\\mA}\\hat{\\vb} \\hat{\\mA}\\hat{\\mA}^\\intercal\\vx = \\hat{\\mA}\\hat{\\vb} , which simplifies to (\\mI + \\gamma\\mD\\mD^\\intercal)\\vx = \\vb. (\\mI + \\gamma\\mD\\mD^\\intercal)\\vx = \\vb. Regularized least squares (aka Tikhonov) \u00b6 We now generalize the result to noisy linear observations of a signal. In this case, the model is \\vb = \\mA\\vx + \\vw, \\vb = \\mA\\vx + \\vw, where we added the measurement matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The corresponding wighted-sum least squares program is \\begin{equation} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 + \\frac{\\gamma}{2}\\|\\mD\\vx\\|_2^2, \\end{equation} \\begin{equation} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 + \\frac{\\gamma}{2}\\|\\mD\\vx\\|_2^2, \\end{equation} where \\|\\mD\\vx\\|_2^2 \\|\\mD\\vx\\|_2^2 is called the regularization penalty and \\gamma \\gamma is called the regularization parameter. The objective function can be reformulated as an least squares objective \\|\\mA\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mA\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. \\|\\mA\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mA\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. and the corresponding normal equations is (\\mA\\mA^\\intercal + \\gamma\\mD\\mD^\\intercal)\\vx = \\vb. (\\mA\\mA^\\intercal + \\gamma\\mD\\mD^\\intercal)\\vx = \\vb.","title":"Regularized least squares"},{"location":"notes/Regularized_LS/#regularized-least-squares","text":"In the least squares problem, we minimized 2-norm squared of the data misfit relative to a linear model. In contrast to the least squares formulation, many problem need to balance competing objectives. For example, consider the problem of finding \\vx_0 \\in \\R^n \\vx_0 \\in \\R^n from noisy linear measurements \\vb = \\mA\\vx_0 + \\vw_\\vb \\vb = \\mA\\vx_0 + \\vw_\\vb and \\vg = \\mF\\vx_0-\\vw_\\vg \\vg = \\mF\\vx_0-\\vw_\\vg . Here \\vw_\\vb \\vw_\\vb and \\vw_\\vg \\vw_\\vg are noise vectors. In order to solve this problem, we need to find a \\vx \\vx that makes \\func{f}_1(\\vx) = \\|\\mA\\vx - \\vb\\|_2^2 \\func{f}_1(\\vx) = \\|\\mA\\vx - \\vb\\|_2^2 small, and \\func{f}_2(\\vx) = \\|\\mB\\vx - \\vg\\|_2^2 \\func{f}_2(\\vx) = \\|\\mB\\vx - \\vg\\|_2^2 small. Generally, we can make \\func{f}_1(\\vx) \\func{f}_1(\\vx) or \\func{f}_2(\\vx) \\func{f}_2(\\vx) small, but not both. The figure below shows this relationship between \\func{f}_1(\\vx) \\func{f}_1(\\vx) and \\func{f}_2(\\vx) \\func{f}_2(\\vx) . In the figure, the points in the boundary of two regions are called the Pareto optimal solutions. In order to find these optimal solutions, we minimize the following weighted sum objective: \\begin{equation}\\label{Regularized_LS_weight} f_1(\\vx)+\\gamma f_2(\\vx) = \\|\\mA\\vx-\\vg\\|_2^2+\u03b3\\|\\mF\\vx-\\vg\\|_2^2. \\end{equation} \\begin{equation}\\label{Regularized_LS_weight} f_1(\\vx)+\\gamma f_2(\\vx) = \\|\\mA\\vx-\\vg\\|_2^2+\u03b3\\|\\mF\\vx-\\vg\\|_2^2. \\end{equation} The parameter \\gamma \\gamma is non-negative and defines relative weight between the objectives. For example, in the case of \\gamma =1 \\gamma =1 , the optimal point that minimizes \\eqref{Regularized_LS_weight} is the point \\vx \\vx on the optimal trade-off curve with f_1(\\vx) = f_2(\\vx) f_1(\\vx) = f_2(\\vx) . Note that for a fixed \\gamma \\gamma and \\alpha \\in \\R \\alpha \\in \\R , the set \\ell(\\gamma,\\alpha) = \\{(f_1(\\vx),f_2(\\vx)):f_1(\\vx) +\\gamma f_2(\\vx) = \\alpha, \\vx \\in \\R^n\\} \\ell(\\gamma,\\alpha) = \\{(f_1(\\vx),f_2(\\vx)):f_1(\\vx) +\\gamma f_2(\\vx) = \\alpha, \\vx \\in \\R^n\\} correspond to a line with slope of -\\gamma -\\gamma . Another way to visualize the optimal solution is to find the line that is tangent to the optimal trade-off cuve, see figure below.","title":"Regularized least squares"},{"location":"notes/Regularized_LS/#example-signal-denoising","text":"Suppose we observe noisy measurements of a signal: \\vb = \\hat{\\vx} + \\vw, \\vb = \\hat{\\vx} + \\vw, where \\hat{\\vx}\\in\\R^n \\hat{\\vx}\\in\\R^n is the signal and \\vw \\in \\R^n \\vw \\in \\R^n is noise. A simple apporach to find \\hat{\\vx} \\hat{\\vx} is to solve: \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2. \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2. This minimization program doesnot enforce any structure on \\vx \\vx . However, if we have prior information that the signal is \"smooth\", then we might balance the least squares fit against the smoothness of the solution in the following way: \\begin{equation}\\label{Regularized_LS_identity} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2 +\\frac{\\gamma}{2} \\underbrace{\\sum_{i=1}^{n-1}(x_i - x_{i+})^2}_{f_2(\\vx)}. \\end{equation} \\begin{equation}\\label{Regularized_LS_identity} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2 +\\frac{\\gamma}{2} \\underbrace{\\sum_{i=1}^{n-1}(x_i - x_{i+})^2}_{f_2(\\vx)}. \\end{equation} Here, f_2(x) f_2(x) promotes smoothness. We can alternatively write the above minimization program in matrix notation. Define the finite differencem matrix \\mD = \\bmat 1 & -1 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & -1 & 0 & \\cdots & 0\\\\ & & \\ddots & \\ddots & & \\\\ 0 & \\cdots & 0 & 1 & -1 & 0\\\\ 0 & 0 & \\cdots & 0 & 1 & -1\\emat \\in \\R^{{n-1}\\times n} \\mD = \\bmat 1 & -1 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & -1 & 0 & \\cdots & 0\\\\ & & \\ddots & \\ddots & & \\\\ 0 & \\cdots & 0 & 1 & -1 & 0\\\\ 0 & 0 & \\cdots & 0 & 1 & -1\\emat \\in \\R^{{n-1}\\times n} So, we can rewrite f_2(\\vx) = \\sum_{i=1}^{n-1}(x_i - x_{i+})^2 = \\|\\mD\\vx\\|_2^2 f_2(\\vx) = \\sum_{i=1}^{n-1}(x_i - x_{i+})^2 = \\|\\mD\\vx\\|_2^2 . This allows for a reformulation of the weighted leas squares objective into a familiar least squares objective: \\|\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mI\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. \\|\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mI\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. So the solution to the weighted least squares minimization program \\eqref{Regularized_LS_identity} satisfies the normal equation \\hat{\\mA}\\hat{\\mA}^\\intercal\\vx = \\hat{\\mA}\\hat{\\vb} \\hat{\\mA}\\hat{\\mA}^\\intercal\\vx = \\hat{\\mA}\\hat{\\vb} , which simplifies to (\\mI + \\gamma\\mD\\mD^\\intercal)\\vx = \\vb. (\\mI + \\gamma\\mD\\mD^\\intercal)\\vx = \\vb.","title":"Example: Signal denoising"},{"location":"notes/Regularized_LS/#regularized-least-squares-aka-tikhonov","text":"We now generalize the result to noisy linear observations of a signal. In this case, the model is \\vb = \\mA\\vx + \\vw, \\vb = \\mA\\vx + \\vw, where we added the measurement matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The corresponding wighted-sum least squares program is \\begin{equation} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 + \\frac{\\gamma}{2}\\|\\mD\\vx\\|_2^2, \\end{equation} \\begin{equation} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 + \\frac{\\gamma}{2}\\|\\mD\\vx\\|_2^2, \\end{equation} where \\|\\mD\\vx\\|_2^2 \\|\\mD\\vx\\|_2^2 is called the regularization penalty and \\gamma \\gamma is called the regularization parameter. The objective function can be reformulated as an least squares objective \\|\\mA\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mA\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. \\|\\mA\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mA\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. and the corresponding normal equations is (\\mA\\mA^\\intercal + \\gamma\\mD\\mD^\\intercal)\\vx = \\vb. (\\mA\\mA^\\intercal + \\gamma\\mD\\mD^\\intercal)\\vx = \\vb.","title":"Regularized least squares (aka Tikhonov)"},{"location":"notes/background/","text":"Mathematical background \u00b6 Vectors \u00b6 Vectors are arrays of numbers ordered as a column. For example, a vector \\vx \\in \\R^n \\vx \\in \\R^n contain n n entries with x_i x_i as its i i th entry: \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. Some special vectors that are commonly used are the zero vector, ones vector, and the standard basis vectors. These are defined next: Zero vector: A vector \\vx \\vx is the zero vector if all of its entries are zero. The zero vector is denoted by \\vzero \\vzero . Ones vector: A vector \\vx \\vx is the ones vector if all of its entries are one. The ones vector is denoted by \\ve \\ve . i i th standard basis vector: A vector \\vx \\in \\R^n \\vx \\in \\R^n is the i i th standard basis vector if x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} . The i i th standard basis vector is denoted by \\ve_i \\ve_i . For example, in \\R^3 \\R^3 , the 2 2 nd standard basis vector is \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} . A vector \\vx \\in \\R^n \\vx \\in \\R^n is equal to another vector \\vy \\in \\R^n \\vy \\in \\R^n if x_i = y_i x_i = y_i for all i \\in \\{1,2,\\dots,n\\} i \\in \\{1,2,\\dots,n\\} . Similarly, if all of the entries of a vector \\vx \\vx is equal to a scalar c \\in \\R c \\in \\R , then we write \\vx = c \\vx = c . These notions can be extended to inequalites in the following way: A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx \\geq c \\vx \\geq c if and only if x_i \\geq c x_i \\geq c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx > c \\vx > c if and only if x_i > c x_i > c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Block vector is a concise representation of a vector obtained by stacking vectors. For example, if \\vx \\in \\R^m \\vx \\in \\R^m , \\vy \\in \\R^n \\vy \\in \\R^n , and \\vz \\in \\R^p \\vz \\in \\R^p , then \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} is a block vector (also called stacked vector or conncatenated vector) obtained by stacking \\vx \\vx , \\vy \\vy , and \\vz \\vz vertically. This block vector \\va \\va can also be written as \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} . If the dimension of the vectors are equal (i.e. m = n = p m = n = p ), then these vector can be stacked horizontally as well. This will produce a matrix \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} where \\vx,\\vy \\vx,\\vy , and \\vz \\vz are the first, second, and third columns of \\mB \\mB , respectively. Matrices \u00b6 A matrix is an array of numbers and is another way of collecting numbers. For example, a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} contain n n rows and m m columns with a total of m\\cdot n m\\cdot n number of entries: \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. The tranpose of this matrix \\mA \\mA is denoted by \\mA^\\intercal\\in\\R^{n\\times m} \\mA^\\intercal\\in\\R^{n\\times m} where \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} A matrix \\mA \\mA is the zero matrix if all of its entrie are zero. The zero matrix is denoted by \\mzero \\mzero . For a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} , if m>n m>n then the matrix \\mA \\mA is a tall matrix, if m< n m< n then the matrix \\mA \\mA is a wide matrix, and if m = n m = n then the matrix \\mA \\mA is a square matrix. Matrices \\mA \\mA and \\mB \\mB are equal if these matrices are of same size and every element is equal: $$ \\mA = \\mB \\iff A_{ij} = B_{ij},\\quad \\forall i = 1,\\dots,m, \\quad j = 1,\\dots,n. $$ If all of the entries of a matrix \\mA \\mA are a scalar c \\in \\R c \\in \\R then we write \\mA = c \\mA = c . These notions are extended to inequalities in the following way: \\mA \\geq c \\mA \\geq c if and only if A_{ij} \\geq c A_{ij} \\geq c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > c \\mA > c if and only if A_{ij} > c A_{ij} > c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA \\geq \\mB \\mA \\geq \\mB if and only if these matrices are of same size and A_{ij} \\geq B_{ij} A_{ij} \\geq B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > \\mB \\mA > \\mB if and only if these matrices are of same size and A_{ij} > B_{ij} A_{ij} > B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. Inner products \u00b6 Inner product is a way to multiply two vectors and matrices to produce a scalar. For \\setV = \\R^n\\text{ or } \\R^{m\\times n} \\setV = \\R^n\\text{ or } \\R^{m\\times n} , inner product is a map \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R that satisfies the following properties for all \\vx, \\vy, \\vz \\in \\setV \\vx, \\vy, \\vz \\in \\setV and \\alpha \\in \\R \\alpha \\in \\R : Non-negativity: \\langle \\vx,\\vx\\rangle \\geq 0 \\langle \\vx,\\vx\\rangle \\geq 0 and \\langle \\vx,\\vx\\rangle = 0 \\langle \\vx,\\vx\\rangle = 0 if and only if \\vx = 0 \\vx = 0 . Symmetry: \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle . Linearity: \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle and \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle . For \\setV = \\R^n \\setV = \\R^n , the Euclidean inner product between two vector \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n is also denoted as \\vx^\\intercal\\vy \\vx^\\intercal\\vy and \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\exa{1} \\exa{1} Fix i \\in \\{1,\\dots,n\\} i \\in \\{1,\\dots,n\\} . The inner product of i i th standard basis vector \\ve_i \\ve_i with an arbitary vector \\vx \\vx is the i i th entry of \\vx \\vx , i.e. \\ve_i^\\intercal\\vx = x_i \\ve_i^\\intercal\\vx = x_i . \\exa{2} \\exa{2} The inner product of the vector of ones \\ve \\ve with an arbitary vector \\vx \\vx is in the sum of entres of \\vx \\vx , i.e. \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n . \\exa{3} \\exa{3} The inner product on an arbitary vector \\vx \\vx with itself is the sum of squares of the entries of \\vx \\vx , i.e. \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 . \\exa{4} \\exa{4} Let \\vp = (p_1,p_2,\\dots,p_n) \\vp = (p_1,p_2,\\dots,p_n) be a vector of prices and \\vq = (q_1,q_2,\\dots,q_n) \\vq = (q_1,q_2,\\dots,q_n) be the corresponding vector of quantities. The total cost is the inner product of \\vp \\vp and \\vq \\vq , i.e. \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i . \\exa{5} \\exa{5} Let \\vw \\vw be a vector of assest allocations ( \\ve^\\intercal\\vw = 1, \\vw\\geq 0 \\ve^\\intercal\\vw = 1, \\vw\\geq 0 ) and \\vp \\vp be the corresponding vector of asset prices. The total portfolio value is \\vw^\\intercal\\vp \\vw^\\intercal\\vp . For \\setV = \\R^{m\\times n} \\setV = \\R^{m\\times n} , the trace inner product between two matrices \\mA, \\mB \\in \\R^{m\\times n} \\mA, \\mB \\in \\R^{m\\times n} is also denoted as \\text{tr}(\\mA^\\intercal\\mB) \\text{tr}(\\mA^\\intercal\\mB) and \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, where for a square matrix \\mS \\in \\R^{n\\times n} \\mS \\in \\R^{n\\times n} , \\trace{\\mS} =\\sum_{i=1}^n S_{ii} \\trace{\\mS} =\\sum_{i=1}^n S_{ii} , and for a matrix \\mA \\mA with columns \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m , \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}. \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}. Norms \u00b6 Norms are a measure of distances of a vector from the origin. The most common norm is the \"Euclidean norm\", i.e. 2-norm: \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} This 2-norm is the norm induced by the Euclidean inner product on \\R^n \\R^n , where \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} . The 2-norm reults in the familiar pythagorean identity, i.e. for any vectors \\va, \\vb, \\va, \\vb, and \\vc \\vc , \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. An important inequlity that relates the inner product of two arbitary vectors with the norms of those vectors induced by the inner product is the Cauchy-Schwartz inequality. In the case of \\R^n \\R^n , the Cauchy-Schwartz inequality states that for all \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n , we have \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. For \\R^n \\R^n , the Cauchy Schwartz inequality follows from the Cosine inequality, i.e. for any vectors \\vx, \\vy \\vx, \\vy with angle \\theta \\theta between \\vx \\vx and \\vy \\vy , we have \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) . So, the Cauchy-Schwartz inequality attains the equality if and only if the two vectors are co-linear. The Cauchy-Schwartz inequality gives us another inequailty that norms, in general, satisfy. This inequailty is the triangle inequality and states that \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\proof \\proof We will use Cauchy-Schwartz inequality to prove \\eqref{background:triangle}. Consider \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} where the second equality follows from linearty of inner product and the inequality follows from the Cauchy-Schwartz inequality. q.e.d. More generally, norms are any function \\|\\cdot\\|:\\R^n \\rightarrow \\R \\|\\cdot\\|:\\R^n \\rightarrow \\R that satisfy the following conditions for all \\vx,\\vy \\in\\R^n \\vx,\\vy \\in\\R^n and \\beta \\in \\R \\beta \\in \\R : Homogeneity: \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| . Triangle inequality: \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| . Non-negativity: \\|\\vx\\|\\geq 0 \\|\\vx\\|\\geq 0 and \\|\\vx\\|=0 \\|\\vx\\|=0 if and only if \\vx = \\vzero \\vx = \\vzero . Other useful norms that satify the above conditions are: 1-norm: The 1-norm of \\vx \\vx is \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| . Sup-norm: The sup-norm of \\vx \\vx is the largest entry of \\vx \\vx in absolute value, i.e. \\|\\vx\\|_\\infty = \\max_{j}|x_j| \\|\\vx\\|_\\infty = \\max_{j}|x_j| . p-norm: For p\\geq 1 p\\geq 1 , the p-norm of \\vx \\vx is \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} . Linear functions \u00b6 Linear function in a mapping between two Eucliean spaces that preserve the operations of addition and scalar multiplication. Specifially, a function \\func{f}:\\R^n\\rightarrow\\R^m \\func{f}:\\R^n\\rightarrow\\R^m is linear if for all \\vx,\\vy\\in\\R^n \\vx,\\vy\\in\\R^n and \\alpha\\in\\R \\alpha\\in\\R , we have \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) Here, \\func{f} \\func{f} takes a vector of dimension n n as its input and outputs a vector of dimension of m m . If m=1 m=1 , then we say the function \\func{f} \\func{f} is a real valued function. \\prop \\prop A real valued function \\func{f}:\\R^n\\rightarrow \\R \\func{f}:\\R^n\\rightarrow \\R is linear if and only if \\func{f} = \\va^\\intercal\\vx \\func{f} = \\va^\\intercal\\vx for some \\va \\in\\R^n \\va \\in\\R^n . \\proof \\proof We will first prove the forward direction, i.e. if \\func{f}:\\R^n\\rightarrow\\R \\func{f}:\\R^n\\rightarrow\\R is a linear function then \\func{f}(\\vx)=\\va^\\intercal\\vx \\func{f}(\\vx)=\\va^\\intercal\\vx . For i\\in\\{1,\\dots,n\\} i\\in\\{1,\\dots,n\\} , let a_{i} = \\func{f}(\\ve_i) a_{i} = \\func{f}(\\ve_i) . Then since \\vx = x_1\\ve_1+\\dots+x_n\\ve_n \\vx = x_1\\ve_1+\\dots+x_n\\ve_n , we have \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} Second, we prove the reverse direction, i.e. if \\func{f}(\\vx) = \\va^\\intercal\\vx \\func{f}(\\vx) = \\va^\\intercal\\vx for some \\va \\in \\R^n \\va \\in \\R^n then \\func{f} \\func{f} is a real valued linear function. Clearly, \\func{f}(x) \\func{f}(x) is a real valued function. Let \\vx,\\vy\\in \\R^n \\vx,\\vy\\in \\R^n and \\alpha \\in \\R \\alpha \\in \\R . Consider \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} where the second inequality follows from linearity of inner product. q.e.d.","title":"Mathematical background"},{"location":"notes/background/#mathematical-background","text":"","title":"Mathematical background"},{"location":"notes/background/#vectors","text":"Vectors are arrays of numbers ordered as a column. For example, a vector \\vx \\in \\R^n \\vx \\in \\R^n contain n n entries with x_i x_i as its i i th entry: \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. Some special vectors that are commonly used are the zero vector, ones vector, and the standard basis vectors. These are defined next: Zero vector: A vector \\vx \\vx is the zero vector if all of its entries are zero. The zero vector is denoted by \\vzero \\vzero . Ones vector: A vector \\vx \\vx is the ones vector if all of its entries are one. The ones vector is denoted by \\ve \\ve . i i th standard basis vector: A vector \\vx \\in \\R^n \\vx \\in \\R^n is the i i th standard basis vector if x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} . The i i th standard basis vector is denoted by \\ve_i \\ve_i . For example, in \\R^3 \\R^3 , the 2 2 nd standard basis vector is \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} . A vector \\vx \\in \\R^n \\vx \\in \\R^n is equal to another vector \\vy \\in \\R^n \\vy \\in \\R^n if x_i = y_i x_i = y_i for all i \\in \\{1,2,\\dots,n\\} i \\in \\{1,2,\\dots,n\\} . Similarly, if all of the entries of a vector \\vx \\vx is equal to a scalar c \\in \\R c \\in \\R , then we write \\vx = c \\vx = c . These notions can be extended to inequalites in the following way: A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx \\geq c \\vx \\geq c if and only if x_i \\geq c x_i \\geq c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx > c \\vx > c if and only if x_i > c x_i > c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Block vector is a concise representation of a vector obtained by stacking vectors. For example, if \\vx \\in \\R^m \\vx \\in \\R^m , \\vy \\in \\R^n \\vy \\in \\R^n , and \\vz \\in \\R^p \\vz \\in \\R^p , then \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} is a block vector (also called stacked vector or conncatenated vector) obtained by stacking \\vx \\vx , \\vy \\vy , and \\vz \\vz vertically. This block vector \\va \\va can also be written as \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} . If the dimension of the vectors are equal (i.e. m = n = p m = n = p ), then these vector can be stacked horizontally as well. This will produce a matrix \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} where \\vx,\\vy \\vx,\\vy , and \\vz \\vz are the first, second, and third columns of \\mB \\mB , respectively.","title":"Vectors"},{"location":"notes/background/#matrices","text":"A matrix is an array of numbers and is another way of collecting numbers. For example, a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} contain n n rows and m m columns with a total of m\\cdot n m\\cdot n number of entries: \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. The tranpose of this matrix \\mA \\mA is denoted by \\mA^\\intercal\\in\\R^{n\\times m} \\mA^\\intercal\\in\\R^{n\\times m} where \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} A matrix \\mA \\mA is the zero matrix if all of its entrie are zero. The zero matrix is denoted by \\mzero \\mzero . For a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} , if m>n m>n then the matrix \\mA \\mA is a tall matrix, if m< n m< n then the matrix \\mA \\mA is a wide matrix, and if m = n m = n then the matrix \\mA \\mA is a square matrix. Matrices \\mA \\mA and \\mB \\mB are equal if these matrices are of same size and every element is equal: $$ \\mA = \\mB \\iff A_{ij} = B_{ij},\\quad \\forall i = 1,\\dots,m, \\quad j = 1,\\dots,n. $$ If all of the entries of a matrix \\mA \\mA are a scalar c \\in \\R c \\in \\R then we write \\mA = c \\mA = c . These notions are extended to inequalities in the following way: \\mA \\geq c \\mA \\geq c if and only if A_{ij} \\geq c A_{ij} \\geq c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > c \\mA > c if and only if A_{ij} > c A_{ij} > c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA \\geq \\mB \\mA \\geq \\mB if and only if these matrices are of same size and A_{ij} \\geq B_{ij} A_{ij} \\geq B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > \\mB \\mA > \\mB if and only if these matrices are of same size and A_{ij} > B_{ij} A_{ij} > B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n.","title":"Matrices"},{"location":"notes/background/#inner-products","text":"Inner product is a way to multiply two vectors and matrices to produce a scalar. For \\setV = \\R^n\\text{ or } \\R^{m\\times n} \\setV = \\R^n\\text{ or } \\R^{m\\times n} , inner product is a map \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R that satisfies the following properties for all \\vx, \\vy, \\vz \\in \\setV \\vx, \\vy, \\vz \\in \\setV and \\alpha \\in \\R \\alpha \\in \\R : Non-negativity: \\langle \\vx,\\vx\\rangle \\geq 0 \\langle \\vx,\\vx\\rangle \\geq 0 and \\langle \\vx,\\vx\\rangle = 0 \\langle \\vx,\\vx\\rangle = 0 if and only if \\vx = 0 \\vx = 0 . Symmetry: \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle . Linearity: \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle and \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle . For \\setV = \\R^n \\setV = \\R^n , the Euclidean inner product between two vector \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n is also denoted as \\vx^\\intercal\\vy \\vx^\\intercal\\vy and \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\exa{1} \\exa{1} Fix i \\in \\{1,\\dots,n\\} i \\in \\{1,\\dots,n\\} . The inner product of i i th standard basis vector \\ve_i \\ve_i with an arbitary vector \\vx \\vx is the i i th entry of \\vx \\vx , i.e. \\ve_i^\\intercal\\vx = x_i \\ve_i^\\intercal\\vx = x_i . \\exa{2} \\exa{2} The inner product of the vector of ones \\ve \\ve with an arbitary vector \\vx \\vx is in the sum of entres of \\vx \\vx , i.e. \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n . \\exa{3} \\exa{3} The inner product on an arbitary vector \\vx \\vx with itself is the sum of squares of the entries of \\vx \\vx , i.e. \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 . \\exa{4} \\exa{4} Let \\vp = (p_1,p_2,\\dots,p_n) \\vp = (p_1,p_2,\\dots,p_n) be a vector of prices and \\vq = (q_1,q_2,\\dots,q_n) \\vq = (q_1,q_2,\\dots,q_n) be the corresponding vector of quantities. The total cost is the inner product of \\vp \\vp and \\vq \\vq , i.e. \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i . \\exa{5} \\exa{5} Let \\vw \\vw be a vector of assest allocations ( \\ve^\\intercal\\vw = 1, \\vw\\geq 0 \\ve^\\intercal\\vw = 1, \\vw\\geq 0 ) and \\vp \\vp be the corresponding vector of asset prices. The total portfolio value is \\vw^\\intercal\\vp \\vw^\\intercal\\vp . For \\setV = \\R^{m\\times n} \\setV = \\R^{m\\times n} , the trace inner product between two matrices \\mA, \\mB \\in \\R^{m\\times n} \\mA, \\mB \\in \\R^{m\\times n} is also denoted as \\text{tr}(\\mA^\\intercal\\mB) \\text{tr}(\\mA^\\intercal\\mB) and \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, where for a square matrix \\mS \\in \\R^{n\\times n} \\mS \\in \\R^{n\\times n} , \\trace{\\mS} =\\sum_{i=1}^n S_{ii} \\trace{\\mS} =\\sum_{i=1}^n S_{ii} , and for a matrix \\mA \\mA with columns \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m , \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}. \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}.","title":"Inner products"},{"location":"notes/background/#norms","text":"Norms are a measure of distances of a vector from the origin. The most common norm is the \"Euclidean norm\", i.e. 2-norm: \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} This 2-norm is the norm induced by the Euclidean inner product on \\R^n \\R^n , where \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} . The 2-norm reults in the familiar pythagorean identity, i.e. for any vectors \\va, \\vb, \\va, \\vb, and \\vc \\vc , \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. An important inequlity that relates the inner product of two arbitary vectors with the norms of those vectors induced by the inner product is the Cauchy-Schwartz inequality. In the case of \\R^n \\R^n , the Cauchy-Schwartz inequality states that for all \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n , we have \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. For \\R^n \\R^n , the Cauchy Schwartz inequality follows from the Cosine inequality, i.e. for any vectors \\vx, \\vy \\vx, \\vy with angle \\theta \\theta between \\vx \\vx and \\vy \\vy , we have \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) . So, the Cauchy-Schwartz inequality attains the equality if and only if the two vectors are co-linear. The Cauchy-Schwartz inequality gives us another inequailty that norms, in general, satisfy. This inequailty is the triangle inequality and states that \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\proof \\proof We will use Cauchy-Schwartz inequality to prove \\eqref{background:triangle}. Consider \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} where the second equality follows from linearty of inner product and the inequality follows from the Cauchy-Schwartz inequality. q.e.d. More generally, norms are any function \\|\\cdot\\|:\\R^n \\rightarrow \\R \\|\\cdot\\|:\\R^n \\rightarrow \\R that satisfy the following conditions for all \\vx,\\vy \\in\\R^n \\vx,\\vy \\in\\R^n and \\beta \\in \\R \\beta \\in \\R : Homogeneity: \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| . Triangle inequality: \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| . Non-negativity: \\|\\vx\\|\\geq 0 \\|\\vx\\|\\geq 0 and \\|\\vx\\|=0 \\|\\vx\\|=0 if and only if \\vx = \\vzero \\vx = \\vzero . Other useful norms that satify the above conditions are: 1-norm: The 1-norm of \\vx \\vx is \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| . Sup-norm: The sup-norm of \\vx \\vx is the largest entry of \\vx \\vx in absolute value, i.e. \\|\\vx\\|_\\infty = \\max_{j}|x_j| \\|\\vx\\|_\\infty = \\max_{j}|x_j| . p-norm: For p\\geq 1 p\\geq 1 , the p-norm of \\vx \\vx is \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} .","title":"Norms"},{"location":"notes/background/#linear-functions","text":"Linear function in a mapping between two Eucliean spaces that preserve the operations of addition and scalar multiplication. Specifially, a function \\func{f}:\\R^n\\rightarrow\\R^m \\func{f}:\\R^n\\rightarrow\\R^m is linear if for all \\vx,\\vy\\in\\R^n \\vx,\\vy\\in\\R^n and \\alpha\\in\\R \\alpha\\in\\R , we have \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) Here, \\func{f} \\func{f} takes a vector of dimension n n as its input and outputs a vector of dimension of m m . If m=1 m=1 , then we say the function \\func{f} \\func{f} is a real valued function. \\prop \\prop A real valued function \\func{f}:\\R^n\\rightarrow \\R \\func{f}:\\R^n\\rightarrow \\R is linear if and only if \\func{f} = \\va^\\intercal\\vx \\func{f} = \\va^\\intercal\\vx for some \\va \\in\\R^n \\va \\in\\R^n . \\proof \\proof We will first prove the forward direction, i.e. if \\func{f}:\\R^n\\rightarrow\\R \\func{f}:\\R^n\\rightarrow\\R is a linear function then \\func{f}(\\vx)=\\va^\\intercal\\vx \\func{f}(\\vx)=\\va^\\intercal\\vx . For i\\in\\{1,\\dots,n\\} i\\in\\{1,\\dots,n\\} , let a_{i} = \\func{f}(\\ve_i) a_{i} = \\func{f}(\\ve_i) . Then since \\vx = x_1\\ve_1+\\dots+x_n\\ve_n \\vx = x_1\\ve_1+\\dots+x_n\\ve_n , we have \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} Second, we prove the reverse direction, i.e. if \\func{f}(\\vx) = \\va^\\intercal\\vx \\func{f}(\\vx) = \\va^\\intercal\\vx for some \\va \\in \\R^n \\va \\in \\R^n then \\func{f} \\func{f} is a real valued linear function. Clearly, \\func{f}(x) \\func{f}(x) is a real valued function. Let \\vx,\\vy\\in \\R^n \\vx,\\vy\\in \\R^n and \\alpha \\in \\R \\alpha \\in \\R . Consider \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} where the second inequality follows from linearity of inner product. q.e.d.","title":"Linear functions"},{"location":"notes/introduction/","text":"Introduction \u00b6 This is the introduction to the class. We will cover unconstrained and constrained optimization. Modelling \u00b6 This is how you model. See eq 1 Let's create another reference to optimimality","title":"Introduction"},{"location":"notes/introduction/#introduction","text":"This is the introduction to the class. We will cover unconstrained and constrained optimization.","title":"Introduction"},{"location":"notes/introduction/#modelling","text":"This is how you model. See eq 1 Let's create another reference to optimimality","title":"Modelling"},{"location":"notes/unconstrained/","text":"Unconstrained Optimization \u00b6 In this lecture we consider unconstrained optimization given by \\begin{equation}\\label{unconstrained_prob} \\mathop{\\text{minimize}}_{\\vx \\in \\set{S}} f(\\vx),\\end{equation} \\begin{equation}\\label{unconstrained_prob} \\mathop{\\text{minimize}}_{\\vx \\in \\set{S}} f(\\vx),\\end{equation} where f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R and \\set{S}\\subset \\R^n \\set{S}\\subset \\R^n . In the course, we will primarily consider minimization problems. This is because a maximizer of f(\\vx) f(\\vx) is the minimzer of -f(\\vx) -f(\\vx) . Optimality \u00b6 A point \\vx^*\\in \\set{S} \\vx^*\\in \\set{S} is a global minimizer of f f if f(\\vx^*) \\leq f(\\vx) f(\\vx^*) \\leq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS global maximizer of f f if f(\\vx^*) \\geq f(\\vx) f(\\vx^*) \\geq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS local minimizer of f f if f(\\vx^*) \\leq f(\\vx) f(\\vx^*) \\leq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS where \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon local maximizer of f f if f(\\vx^*) \\geq f(\\vx) f(\\vx^*) \\geq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS where \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon for some \\epsilon > 0 \\epsilon > 0 . A maximizer (or minimizer) \\vx^* \\vx^* of \\eqref{unconstrained_prob} is a strict maximizer (or minimizer) if for all points in a neighborhood of \\vx^* \\vx^* , the objective f(x) f(x) does not attain the value of f(\\vx^{*}) f(\\vx^{*}) . Equivalently, this definition of strict maximizer/minimizer in the global case can be stated as: \\vx^* \\vx^* is a strict global min or max if for all \\vx\\in \\set{S} \\vx\\in \\set{S} , we have f(\\vx^*) = f(\\vx) f(\\vx^*) = f(\\vx) if and only if \\vx = \\vx^* \\vx = \\vx^* . The above figure shows that even if the optimality is attained, the optimal point may not be unique. There are cases when the optimal point is not attained or even exist. An example of these cases are shown below \\exa{1} \\exa{1} Consider the minimization program \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = x + y: x^2 + y^2 \\leq 2\\}. \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = x + y: x^2 + y^2 \\leq 2\\}. The figure below shows the interaction bewtween the objective fuction and set \\{(x,y):x^2 + y^2 \\leq 2\\} \\{(x,y):x^2 + y^2 \\leq 2\\} . The pink lines are the \"level sets\" and the arrows are directions of descent. The optimal point is \\bmat -1\\\\ -1 \\emat \\bmat -1\\\\ -1 \\emat . \\exa{2} \\exa{2} Consider the minimization program \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = \\frac{x+y}{x^2+y^2+1}:x,y\\in\\R\\}. \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = \\frac{x+y}{x^2+y^2+1}:x,y\\in\\R\\}. The surface and contour plots of f(x,y) f(x,y) are using Plots ; pyplot () f ( x , y ) = ( x + y ) / ( x ^ 2 + y ^ 2 + 1 ) x = range ( - 4 , stop = 4 , length = 100 ) y = range ( - 5 , stop = 5 , length = 100 ) pyplot ( size = ( 700 , 200 )) plot ( plot ( x , y , f , st =: surface , camera = ( 100 , 30 )), plot ( x , y , f , st =: contour , camera = ( 100 , 30 ))) The global minimizer is \\begin{pmatrix} \\frac{1}{\\sqrt{2}},& \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}},& \\frac{1}{\\sqrt{2}} \\end{pmatrix} and the gobal maximizer is \\bmatp -\\frac{1}{\\sqrt{2}}, & -\\frac{1}{\\sqrt{2}} \\ematp \\bmatp -\\frac{1}{\\sqrt{2}}, & -\\frac{1}{\\sqrt{2}} \\ematp . Sufficient conditions in 1-d \u00b6 We will first look at 1-d function f:\\R\u2192\\R f:\\R\u2192\\R and state sufficent conditions for optimality of a point \\vx \\vx in the interrior of the set \\set{S} \\set{S} . Consider the following 1-d function The critial points of f f in the above figure are x \\in \\{ b ,\\ c,\\ d,\\ e\\} x \\in \\{ b ,\\ c,\\ d,\\ e\\} and these points satisfy f'(x) =0 f'(x) =0 . We may be able to use second derivative information to determine if these critial points are minimizer or maximizer. Precisely, for any function f:\\R\\rightarrow\\R f:\\R\\rightarrow\\R , a point x = x^* x = x^* is a local minimizer if f'(x) = 0 f'(x) = 0 and f''(x)>0 f''(x)>0 , and a local maximizer if f'(x) = 0 f'(x) = 0 and f''(x)<0 f''(x)<0 . However, if both f'(x) = 0 f'(x) = 0 and f''(x)=0 f''(x)=0 , there is not enough information to draw any conclusion. As an example, consider f(x) = x^3 f(x) = x^3 and f(x) = x^4 f(x) = x^4 . For both function, f'(x) = 0 f'(x) = 0 and f''(x) = 0 f''(x) = 0 , however x = 0 x = 0 is saddle point of x^3 x^3 and x = 0 x = 0 is a minimizer of x^4 x^4 . A proof of the sufficient condition of optimality follows directly from Taylor approximation of the function f(x) f(x) . Consider the case where f'(x^*) = 0 f'(x^*) = 0 and f''(x^*)>0 f''(x^*)>0 for some x^*\\in\\set{S} x^*\\in\\set{S} . Then by Taylor's theorem of f(x) f(x) at x = x^* x = x^* , we get f(x) = f(x^*) + \\underbrace{f'(x^*)(x-x^*)}_{ =\\ 0} + \\underbrace{\\frac{1}{2}f''(x^*)(x-x^*)^2}_{\\text{strictly positive}} + \\underbrace{O((x-x^*)^3)}_{\\text{small}} f(x) = f(x^*) + \\underbrace{f'(x^*)(x-x^*)}_{ =\\ 0} + \\underbrace{\\frac{1}{2}f''(x^*)(x-x^*)^2}_{\\text{strictly positive}} + \\underbrace{O((x-x^*)^3)}_{\\text{small}} for x x close to x^* x^* . So f(x) > f(x^*) f(x) > f(x^*) for all x x near x^* x^* , which imples x^* x^* is local minimizer. Similarly, we can prove f(x^*)=0 f(x^*)=0 and f''(x^*)<0 f''(x^*)<0 implies x^* x^* is a local maximizer. In the next section we will generalize these sufficient condition for any real valued function f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R . Gradient and Hessian \u00b6 For a differentiable function f:\\R^n\\to\\R f:\\R^n\\to\\R , the gradient of f f at \\vx \\vx is a vector in \\R^n \\R^n . The gradient of f f at \\vx \\vx contain the partial derivative of f f at x x and is given by \\nabla f(x) = \\bmat\\frac{\\partial f(x)}{\\partial x_1}\\\\\\frac{\\partial f(x)}{\\partial x_2}\\\\\\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_n}\\\\\\emat. \\nabla f(x) = \\bmat\\frac{\\partial f(x)}{\\partial x_1}\\\\\\frac{\\partial f(x)}{\\partial x_2}\\\\\\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_n}\\\\\\emat. Similarly, the Hessian of f f at x x is a symmetric matrix in \\R^{n\\times n} \\R^{n\\times n} that contain second partial derivative information and is given by \\nabla^2 f(x) = \\bmat \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_n}\\\\ \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_2}& \\cdots & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_n}\\\\ \\emat \\nabla^2 f(x) = \\bmat \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_n}\\\\ \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_2}& \\cdots & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_n}\\\\ \\emat For example, for a function f(x) = x_1^2 + 8x_1x_2 - 2x_3^3 f(x) = x_1^2 + 8x_1x_2 - 2x_3^3 , its gradiet is \\nabla f(x) = \\bmat 2x_1 + 8x_2\\\\8x_1\\\\-6x_3^2\\emat \\nabla f(x) = \\bmat 2x_1 + 8x_2\\\\8x_1\\\\-6x_3^2\\emat , and its Hessian is \\nabla^2 f(x) = \\bmat 2 & 8 & 0 \\\\ 8 & 0 & 0 \\\\ 0 & 0 & -12x_3\\emat. \\nabla^2 f(x) = \\bmat 2 & 8 & 0 \\\\ 8 & 0 & 0 \\\\ 0 & 0 & -12x_3\\emat.","title":"Unconstrained optimization"},{"location":"notes/unconstrained/#unconstrained-optimization","text":"In this lecture we consider unconstrained optimization given by \\begin{equation}\\label{unconstrained_prob} \\mathop{\\text{minimize}}_{\\vx \\in \\set{S}} f(\\vx),\\end{equation} \\begin{equation}\\label{unconstrained_prob} \\mathop{\\text{minimize}}_{\\vx \\in \\set{S}} f(\\vx),\\end{equation} where f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R and \\set{S}\\subset \\R^n \\set{S}\\subset \\R^n . In the course, we will primarily consider minimization problems. This is because a maximizer of f(\\vx) f(\\vx) is the minimzer of -f(\\vx) -f(\\vx) .","title":"Unconstrained Optimization"},{"location":"notes/unconstrained/#optimality","text":"A point \\vx^*\\in \\set{S} \\vx^*\\in \\set{S} is a global minimizer of f f if f(\\vx^*) \\leq f(\\vx) f(\\vx^*) \\leq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS global maximizer of f f if f(\\vx^*) \\geq f(\\vx) f(\\vx^*) \\geq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS local minimizer of f f if f(\\vx^*) \\leq f(\\vx) f(\\vx^*) \\leq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS where \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon local maximizer of f f if f(\\vx^*) \\geq f(\\vx) f(\\vx^*) \\geq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS where \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon for some \\epsilon > 0 \\epsilon > 0 . A maximizer (or minimizer) \\vx^* \\vx^* of \\eqref{unconstrained_prob} is a strict maximizer (or minimizer) if for all points in a neighborhood of \\vx^* \\vx^* , the objective f(x) f(x) does not attain the value of f(\\vx^{*}) f(\\vx^{*}) . Equivalently, this definition of strict maximizer/minimizer in the global case can be stated as: \\vx^* \\vx^* is a strict global min or max if for all \\vx\\in \\set{S} \\vx\\in \\set{S} , we have f(\\vx^*) = f(\\vx) f(\\vx^*) = f(\\vx) if and only if \\vx = \\vx^* \\vx = \\vx^* . The above figure shows that even if the optimality is attained, the optimal point may not be unique. There are cases when the optimal point is not attained or even exist. An example of these cases are shown below \\exa{1} \\exa{1} Consider the minimization program \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = x + y: x^2 + y^2 \\leq 2\\}. \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = x + y: x^2 + y^2 \\leq 2\\}. The figure below shows the interaction bewtween the objective fuction and set \\{(x,y):x^2 + y^2 \\leq 2\\} \\{(x,y):x^2 + y^2 \\leq 2\\} . The pink lines are the \"level sets\" and the arrows are directions of descent. The optimal point is \\bmat -1\\\\ -1 \\emat \\bmat -1\\\\ -1 \\emat . \\exa{2} \\exa{2} Consider the minimization program \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = \\frac{x+y}{x^2+y^2+1}:x,y\\in\\R\\}. \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = \\frac{x+y}{x^2+y^2+1}:x,y\\in\\R\\}. The surface and contour plots of f(x,y) f(x,y) are using Plots ; pyplot () f ( x , y ) = ( x + y ) / ( x ^ 2 + y ^ 2 + 1 ) x = range ( - 4 , stop = 4 , length = 100 ) y = range ( - 5 , stop = 5 , length = 100 ) pyplot ( size = ( 700 , 200 )) plot ( plot ( x , y , f , st =: surface , camera = ( 100 , 30 )), plot ( x , y , f , st =: contour , camera = ( 100 , 30 ))) The global minimizer is \\begin{pmatrix} \\frac{1}{\\sqrt{2}},& \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}},& \\frac{1}{\\sqrt{2}} \\end{pmatrix} and the gobal maximizer is \\bmatp -\\frac{1}{\\sqrt{2}}, & -\\frac{1}{\\sqrt{2}} \\ematp \\bmatp -\\frac{1}{\\sqrt{2}}, & -\\frac{1}{\\sqrt{2}} \\ematp .","title":"Optimality"},{"location":"notes/unconstrained/#sufficient-conditions-in-1-d","text":"We will first look at 1-d function f:\\R\u2192\\R f:\\R\u2192\\R and state sufficent conditions for optimality of a point \\vx \\vx in the interrior of the set \\set{S} \\set{S} . Consider the following 1-d function The critial points of f f in the above figure are x \\in \\{ b ,\\ c,\\ d,\\ e\\} x \\in \\{ b ,\\ c,\\ d,\\ e\\} and these points satisfy f'(x) =0 f'(x) =0 . We may be able to use second derivative information to determine if these critial points are minimizer or maximizer. Precisely, for any function f:\\R\\rightarrow\\R f:\\R\\rightarrow\\R , a point x = x^* x = x^* is a local minimizer if f'(x) = 0 f'(x) = 0 and f''(x)>0 f''(x)>0 , and a local maximizer if f'(x) = 0 f'(x) = 0 and f''(x)<0 f''(x)<0 . However, if both f'(x) = 0 f'(x) = 0 and f''(x)=0 f''(x)=0 , there is not enough information to draw any conclusion. As an example, consider f(x) = x^3 f(x) = x^3 and f(x) = x^4 f(x) = x^4 . For both function, f'(x) = 0 f'(x) = 0 and f''(x) = 0 f''(x) = 0 , however x = 0 x = 0 is saddle point of x^3 x^3 and x = 0 x = 0 is a minimizer of x^4 x^4 . A proof of the sufficient condition of optimality follows directly from Taylor approximation of the function f(x) f(x) . Consider the case where f'(x^*) = 0 f'(x^*) = 0 and f''(x^*)>0 f''(x^*)>0 for some x^*\\in\\set{S} x^*\\in\\set{S} . Then by Taylor's theorem of f(x) f(x) at x = x^* x = x^* , we get f(x) = f(x^*) + \\underbrace{f'(x^*)(x-x^*)}_{ =\\ 0} + \\underbrace{\\frac{1}{2}f''(x^*)(x-x^*)^2}_{\\text{strictly positive}} + \\underbrace{O((x-x^*)^3)}_{\\text{small}} f(x) = f(x^*) + \\underbrace{f'(x^*)(x-x^*)}_{ =\\ 0} + \\underbrace{\\frac{1}{2}f''(x^*)(x-x^*)^2}_{\\text{strictly positive}} + \\underbrace{O((x-x^*)^3)}_{\\text{small}} for x x close to x^* x^* . So f(x) > f(x^*) f(x) > f(x^*) for all x x near x^* x^* , which imples x^* x^* is local minimizer. Similarly, we can prove f(x^*)=0 f(x^*)=0 and f''(x^*)<0 f''(x^*)<0 implies x^* x^* is a local maximizer. In the next section we will generalize these sufficient condition for any real valued function f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R .","title":"Sufficient conditions in 1-d"},{"location":"notes/unconstrained/#gradient-and-hessian","text":"For a differentiable function f:\\R^n\\to\\R f:\\R^n\\to\\R , the gradient of f f at \\vx \\vx is a vector in \\R^n \\R^n . The gradient of f f at \\vx \\vx contain the partial derivative of f f at x x and is given by \\nabla f(x) = \\bmat\\frac{\\partial f(x)}{\\partial x_1}\\\\\\frac{\\partial f(x)}{\\partial x_2}\\\\\\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_n}\\\\\\emat. \\nabla f(x) = \\bmat\\frac{\\partial f(x)}{\\partial x_1}\\\\\\frac{\\partial f(x)}{\\partial x_2}\\\\\\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_n}\\\\\\emat. Similarly, the Hessian of f f at x x is a symmetric matrix in \\R^{n\\times n} \\R^{n\\times n} that contain second partial derivative information and is given by \\nabla^2 f(x) = \\bmat \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_n}\\\\ \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_2}& \\cdots & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_n}\\\\ \\emat \\nabla^2 f(x) = \\bmat \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_n}\\\\ \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_2}& \\cdots & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_n}\\\\ \\emat For example, for a function f(x) = x_1^2 + 8x_1x_2 - 2x_3^3 f(x) = x_1^2 + 8x_1x_2 - 2x_3^3 , its gradiet is \\nabla f(x) = \\bmat 2x_1 + 8x_2\\\\8x_1\\\\-6x_3^2\\emat \\nabla f(x) = \\bmat 2x_1 + 8x_2\\\\8x_1\\\\-6x_3^2\\emat , and its Hessian is \\nabla^2 f(x) = \\bmat 2 & 8 & 0 \\\\ 8 & 0 & 0 \\\\ 0 & 0 & -12x_3\\emat. \\nabla^2 f(x) = \\bmat 2 & 8 & 0 \\\\ 8 & 0 & 0 \\\\ 0 & 0 & -12x_3\\emat.","title":"Gradient and Hessian"},{"location":"notes/constrained/","text":"Constrained optimization \u00b6 These are the new notes","title":"Constrained optimization"},{"location":"notes/constrained/#constrained-optimization","text":"These are the new notes","title":"Constrained optimization"}]}