{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Computational Optimization \u00b6 This course covers the main algorithms for continuous optimization, including unconstrained and constrained problems, large-scale problems, and duality theory and sensitivity. We will also cover numerical linear algebra operations needed in optimization, including LU, QR, and Cholesky decompositions. Discrete optimization problems are not covered. Lectures \u00b6 Monday, Wednesday, and Friday, 2-3 pm, DMP 110 Teaching staff (2019 Term 2) \u00b6 Co-instructor: Michael P. Friedlander . Office hours: Monday, 3-4p (ICCS X150) Co-instructor: Babhru Joshi. Office hours: Friday, 3-4p (ICCS X150, table 2) Teaching assistant: Zhenan Fan. Office hours: Tuesday, 2-3p (ICCS X150, table 6) Teaching Assistant: Huang Fang. Office hours: Thursday 2-3p (ICCS X150, table 4) Textbook \u00b6 Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MATLAB , Amir Beck (SIAM, 2014). This book is available online through the UBC Library. Course requirements \u00b6 One of CPSC 302, CPSC 303, or MATH 307. Course discussion board \u00b6 Discussion board is hosted on Piazza . Here is the link to enroll.","title":"Home Page"},{"location":"#computational-optimization","text":"This course covers the main algorithms for continuous optimization, including unconstrained and constrained problems, large-scale problems, and duality theory and sensitivity. We will also cover numerical linear algebra operations needed in optimization, including LU, QR, and Cholesky decompositions. Discrete optimization problems are not covered.","title":"Computational Optimization"},{"location":"#lectures","text":"Monday, Wednesday, and Friday, 2-3 pm, DMP 110","title":"Lectures"},{"location":"#teaching-staff-2019-term-2","text":"Co-instructor: Michael P. Friedlander . Office hours: Monday, 3-4p (ICCS X150) Co-instructor: Babhru Joshi. Office hours: Friday, 3-4p (ICCS X150, table 2) Teaching assistant: Zhenan Fan. Office hours: Tuesday, 2-3p (ICCS X150, table 6) Teaching Assistant: Huang Fang. Office hours: Thursday 2-3p (ICCS X150, table 4)","title":"Teaching staff (2019 Term 2)"},{"location":"#textbook","text":"Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MATLAB , Amir Beck (SIAM, 2014). This book is available online through the UBC Library.","title":"Textbook"},{"location":"#course-requirements","text":"One of CPSC 302, CPSC 303, or MATH 307.","title":"Course requirements"},{"location":"#course-discussion-board","text":"Discussion board is hosted on Piazza . Here is the link to enroll.","title":"Course discussion board"},{"location":"grades/","text":"Grades and policies \u00b6 Grade distribution \u00b6 assignments (approximately 5-6): 30% midterm exam: 30% final exam: 40% Collaboration \u00b6 Most homeworks involve programming tasks. You may collaborate and consult with other students in the course, but you must hand in your own assignments and your own code. If you have collaborated or consulted with someone while working on your assignment, you must acknowledge this explicitly in your submitted assignment. If you are unsure about any of these rules, feel free to consult with your instructor or visit the departmental webpage on Collaboration and Plagiarism . Late submissions \u00b6 Each student has a three business-day allowance to use throughout the term. If an assignment is due by Thursday, then submission by Friday counts as one delay; a submission by Monday counts as two delays; a submission by Tuesday counts as three delays (and consumes the entire three-day allowance). Apart from using delays, late submissions are not accepted. Once a solution set has been posted, no more late submissions are permitted; consequently, you may not always be able to use all of your delays. Policies \u00b6 No makeup exam for the midterm of final. If you missed the midterm exam you must document a justification. Midterm exam grade will not be counted if it is lower than your final exam grade. To pass the course you must do the assigned coursework, write the midterm and final exams, pass the final exam, and obtain an overall pass average according to the grading scheme. The instructors reserve the right to modify the grading scheme at any time.","title":"Grades"},{"location":"grades/#grades-and-policies","text":"","title":"Grades and policies"},{"location":"grades/#grade-distribution","text":"assignments (approximately 5-6): 30% midterm exam: 30% final exam: 40%","title":"Grade distribution"},{"location":"grades/#collaboration","text":"Most homeworks involve programming tasks. You may collaborate and consult with other students in the course, but you must hand in your own assignments and your own code. If you have collaborated or consulted with someone while working on your assignment, you must acknowledge this explicitly in your submitted assignment. If you are unsure about any of these rules, feel free to consult with your instructor or visit the departmental webpage on Collaboration and Plagiarism .","title":"Collaboration"},{"location":"grades/#late-submissions","text":"Each student has a three business-day allowance to use throughout the term. If an assignment is due by Thursday, then submission by Friday counts as one delay; a submission by Monday counts as two delays; a submission by Tuesday counts as three delays (and consumes the entire three-day allowance). Apart from using delays, late submissions are not accepted. Once a solution set has been posted, no more late submissions are permitted; consequently, you may not always be able to use all of your delays.","title":"Late submissions"},{"location":"grades/#policies","text":"No makeup exam for the midterm of final. If you missed the midterm exam you must document a justification. Midterm exam grade will not be counted if it is lower than your final exam grade. To pass the course you must do the assigned coursework, write the midterm and final exams, pass the final exam, and obtain an overall pass average according to the grading scheme. The instructors reserve the right to modify the grading scheme at any time.","title":"Policies"},{"location":"schedule/","text":"Course schedule \u00b6 This is a tentative schedule. It will change often. Lecture Date Topic Notes Homework 1 06 Jan Introduction 2 08 Jan Least squares notebook 3 10 Jan QR factorization 4 13 Jan Regularized least squares 5 15 Jan Non-linear least squares 6 17 Jan Unconstrained mptimization 7 20 Jan Descent methods 8 22 Jan Newton's method 9 24 Jan Quasi-Newton's methods 10 27 Jan Convex functions 11 29 Jan Constrained Optimization 12 31 Jan Linear programming and applications 13 3 Feb Simplex methods 14 5 Feb Duality 15 7 Feb 16 10 Feb 17 12 Feb 18 1 Feb 19 24 Feb 20 26 Feb 21 28 Feb 22 2 Mar 23 4 Mar 24 6 Mar 25 9 Mar 26 11 Mar 27 13 Mar 28 16 Mar 29 18 Mar 30 20 Mar 31 23 Mar 32 25 Mar 33 27 Mar 34 30 Mar 35 1 Apr 36 3 Apr 37 6 Apr 38 8 Apr","title":"Schedule"},{"location":"schedule/#course-schedule","text":"This is a tentative schedule. It will change often. Lecture Date Topic Notes Homework 1 06 Jan Introduction 2 08 Jan Least squares notebook 3 10 Jan QR factorization 4 13 Jan Regularized least squares 5 15 Jan Non-linear least squares 6 17 Jan Unconstrained mptimization 7 20 Jan Descent methods 8 22 Jan Newton's method 9 24 Jan Quasi-Newton's methods 10 27 Jan Convex functions 11 29 Jan Constrained Optimization 12 31 Jan Linear programming and applications 13 3 Feb Simplex methods 14 5 Feb Duality 15 7 Feb 16 10 Feb 17 12 Feb 18 1 Feb 19 24 Feb 20 26 Feb 21 28 Feb 22 2 Mar 23 4 Mar 24 6 Mar 25 9 Mar 26 11 Mar 27 13 Mar 28 16 Mar 29 18 Mar 30 20 Mar 31 23 Mar 32 25 Mar 33 27 Mar 34 30 Mar 35 1 Apr 36 3 Apr 37 6 Apr 38 8 Apr","title":"Course schedule"},{"location":"homework/hw1/","text":"CPSC 406: Homework 1 (Due Jan 17) \u00b6 Backsolve Here, we will explore the computational complexity of solving the system \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} when \\mR \\mR is either upper triangular ( R_{ij} = 0 R_{ij} = 0 whenever i > j i > j ) or lower triangular ( R_{ij} = 0 R_{ij} = 0 whenever i < j i < j ). If \\mR \\mR were fully dense, then solving this system takes O(n^3) O(n^3) flops. We will show that when \\mR \\mR is upper or lower triangular, this system takes O(n^2) O(n^2) flops. Assume that the diagonal elements |R_{ii}| > \\epsilon |R_{ii}| > \\epsilon for \\epsilon \\epsilon suitably large in all cases. Consider \\mR \\mR lower triangular, e.g. we solve the system \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_1 x_1 . (This should take O(1) O(1) flops.) Given x_1,...,x_i x_1,...,x_i , show how to find x_{i+1} x_{i+1} . (This should take O(i) O(i) flops.) Putting it all together, we get O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} Now consider \\mR \\mR upper triangular, e.g. we solve the system \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_n x_n . (This should take O(1) O(1) flops.) Given x_{i+1},...,x_n x_{i+1},...,x_n , show how to find x_{i} x_{i} . (This should take O(n-i) O(n-i) flops.) Putting it all together, we get O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} Linear data fit Download data (Heres the csv format of the same data ). Fit the best line f(z) = x_1 + x_2z f(z) = x_1 + x_2z to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) ; that is, find the best approximation of the line f(z) f(z) to y y in the 2-norm sense. Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. Polynomial data fit Using the same data as above, fit the best order- d d polynomial to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) , for d = 2,3,4,5 d = 2,3,4,5 . That is, find x_1,...,x_{d+1} x_1,...,x_{d+1} such that f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d best approximates the data in the 2-norm sense (minimizing \\sum_i (f(z_i)-y_i)^2 \\sum_i (f(z_i)-y_i)^2 ). Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. About how many degrees is needed for a reasonable fit? QR factorization Consider a full rank, underdetermined but consistent linear system \\mA\\vx = \\vb \\mA\\vx = \\vb , where \\mA \\mA is m\\times n m\\times n with m < n m < n . Show how to use the QR factorization to obtain a soution of this system. The following script can be used to generate random matrices in Julia, given dimensions m=10 m=10 and n=20 n=20 : m = 10 n = 20 A = randn ( m , n ) x = randn ( n ) b = A * x Write a Julia code for solving for x x using the procedure outlined in the previous part of the question. Record the runtime using the Julia calls \\texttt{time()} \\texttt{time()} . (Make sure you are not running anything else or it will interfere with the timing results.) Record the runtimes for matrices of sizes (m,n) = (10,20) (m,n) = (10,20) , (100,200) (100,200) , (100,2000) (100,2000) , (100,20000) (100,20000) , and (100,200000) (100,200000) . Compare the runtimes against finding x x using \\vx = \\mA\\backslash \\vb \\vx = \\mA\\backslash \\vb . The underdetermined and consistent linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has infinitely many solutions. For the case where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} is full rank, show how to use the QR factorization to obtain the least norm solution, i.e. find \\vx_{LN} \\vx_{LN} that solves \\min_{\\vx\\in\\R^n} \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb \\min_{\\vx\\in\\R^n} \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb using QR decomposition.","title":"Homework 1"},{"location":"homework/hw1/#cpsc-406-homework-1-due-jan-17","text":"Backsolve Here, we will explore the computational complexity of solving the system \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} when \\mR \\mR is either upper triangular ( R_{ij} = 0 R_{ij} = 0 whenever i > j i > j ) or lower triangular ( R_{ij} = 0 R_{ij} = 0 whenever i < j i < j ). If \\mR \\mR were fully dense, then solving this system takes O(n^3) O(n^3) flops. We will show that when \\mR \\mR is upper or lower triangular, this system takes O(n^2) O(n^2) flops. Assume that the diagonal elements |R_{ii}| > \\epsilon |R_{ii}| > \\epsilon for \\epsilon \\epsilon suitably large in all cases. Consider \\mR \\mR lower triangular, e.g. we solve the system \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_1 x_1 . (This should take O(1) O(1) flops.) Given x_1,...,x_i x_1,...,x_i , show how to find x_{i+1} x_{i+1} . (This should take O(i) O(i) flops.) Putting it all together, we get O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} Now consider \\mR \\mR upper triangular, e.g. we solve the system \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_n x_n . (This should take O(1) O(1) flops.) Given x_{i+1},...,x_n x_{i+1},...,x_n , show how to find x_{i} x_{i} . (This should take O(n-i) O(n-i) flops.) Putting it all together, we get O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} Linear data fit Download data (Heres the csv format of the same data ). Fit the best line f(z) = x_1 + x_2z f(z) = x_1 + x_2z to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) ; that is, find the best approximation of the line f(z) f(z) to y y in the 2-norm sense. Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. Polynomial data fit Using the same data as above, fit the best order- d d polynomial to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) , for d = 2,3,4,5 d = 2,3,4,5 . That is, find x_1,...,x_{d+1} x_1,...,x_{d+1} such that f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d best approximates the data in the 2-norm sense (minimizing \\sum_i (f(z_i)-y_i)^2 \\sum_i (f(z_i)-y_i)^2 ). Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. About how many degrees is needed for a reasonable fit? QR factorization Consider a full rank, underdetermined but consistent linear system \\mA\\vx = \\vb \\mA\\vx = \\vb , where \\mA \\mA is m\\times n m\\times n with m < n m < n . Show how to use the QR factorization to obtain a soution of this system. The following script can be used to generate random matrices in Julia, given dimensions m=10 m=10 and n=20 n=20 : m = 10 n = 20 A = randn ( m , n ) x = randn ( n ) b = A * x Write a Julia code for solving for x x using the procedure outlined in the previous part of the question. Record the runtime using the Julia calls \\texttt{time()} \\texttt{time()} . (Make sure you are not running anything else or it will interfere with the timing results.) Record the runtimes for matrices of sizes (m,n) = (10,20) (m,n) = (10,20) , (100,200) (100,200) , (100,2000) (100,2000) , (100,20000) (100,20000) , and (100,200000) (100,200000) . Compare the runtimes against finding x x using \\vx = \\mA\\backslash \\vb \\vx = \\mA\\backslash \\vb . The underdetermined and consistent linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has infinitely many solutions. For the case where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} is full rank, show how to use the QR factorization to obtain the least norm solution, i.e. find \\vx_{LN} \\vx_{LN} that solves \\min_{\\vx\\in\\R^n} \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb \\min_{\\vx\\in\\R^n} \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb using QR decomposition.","title":"CPSC 406: Homework 1 (Due Jan  17)"},{"location":"notes/Least_squares/","text":"Least Squares \u00b6 In this lecture, we will cover least squares for data fitting, linear systems, properties of least squares and QR factorization. Least squares for data fitting \u00b6 Consider the problem of fitting a line to observations y_i y_i gven input z_i z_i for i = 1,\\dots, n i = 1,\\dots, n . In the figure above, the data points seem to follow a linear trend. One way to find the parameters c,s \\in \\R c,s \\in \\R of the linear model f(z) = s\\cdot z + c f(z) = s\\cdot z + c that coresponds to a line of best fit is to minimize the following squared distance subject to a linear constraint: \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} The above minimization program can be reformulated as a linear least squares problem: \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, where \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. Let \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} The gradient and hessian of \\func{f}(\\vx) \\func{f}(\\vx) are: \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, respectively. The Hessian is positive semidefinite for every \\vx \\vx (and is positive definite if \\mA \\mA has full row rank). This implies that the function \\func{f}(\\vx) \\func{f}(\\vx) is convex. Additionally, \\vx = \\vx^{*} \\vx = \\vx^{*} is a critical point if \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} Since \\func{f}(\\vx) \\func{f}(\\vx) is convex, \\vx^{*} \\vx^{*} is a global minimizer. Equation \\eqref{least_square_normal_eqn} is called the normal equations of the least squares problem \\eqref{least_squares_problem}. Solving the normal equations, we get the following line of best fit. Linear systems \u00b6 Consider the problem of solving a linear system of equations. For \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in\\R^{m} \\vb\\in\\R^{m} , the linear system of equations \\mA\\vx = \\vb \\mA\\vx = \\vb is: overdetermined if m>n m>n , underdetermined if m< n m< n , or square if m = n m = n . A linear system can have exactly one solution, many solutions, or no solutions: In general, a linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has a solution if \\vb \\in \\text{range} (\\mA) \\vb \\in \\text{range} (\\mA) . Properties of linear least squares \u00b6 Recall that the minimizer \\vx^* \\vx^* to the linear least squares poblem satisfies the normal equations: \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb with the residual \\vr^* = \\mA\\vx^* -\\vb, \\vr^* = \\mA\\vx^* -\\vb, satisfying \\mA^\\intercal\\vr^* = \\vzero. \\mA^\\intercal\\vr^* = \\vzero. Here, \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The minimzer of the linear least squares problem is unique if \\mA^\\intercal\\mA \\mA^\\intercal\\mA is invertible. However, the vector in the range of \\mA \\mA closest to \\vb \\vb is unique, i.e. \\vb^* = \\mA\\vx* \\vb^* = \\mA\\vx* is unique. Recall that range space of \\mA \\mA and the null space of \\mA^\\intercal \\mA^\\intercal is: \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} By fundamental theorem of linear algebra, we have $$ \\begin{equation}\\label{least_squares_FTLA} \\set{R}(\\mA) \\oplus \\set{N}(\\mA^\\intercal) = \\R^m. \\end{equation} $$ Thus, for all \\vx \\in \\R^m \\vx \\in \\R^m , we have \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) with \\vu \\vu and \\vv \\vv uniquely determined. This is illustrated in the figure below: Here, \\vx_{LS} \\vx_{LS} is the least squares solution, \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} , with \\va_i \\in \\R^m \\va_i \\in \\R^m for all i i . Comparing with \\eqref{least_squares_FTLA}, we get \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\exa{1} \\exa{1} What is the least-squares solution \\vx^* \\vx^* for the problem \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, where \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\text{Solution:} \\text{Solution:} First setup the normal equations: \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb Solving the normal equations, we get \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb So, the least squares solution \\vx^* \\vx^* is the mean value of the elements in \\vb \\vb .","title":"Least Squares"},{"location":"notes/Least_squares/#least-squares","text":"In this lecture, we will cover least squares for data fitting, linear systems, properties of least squares and QR factorization.","title":"Least Squares"},{"location":"notes/Least_squares/#least-squares-for-data-fitting","text":"Consider the problem of fitting a line to observations y_i y_i gven input z_i z_i for i = 1,\\dots, n i = 1,\\dots, n . In the figure above, the data points seem to follow a linear trend. One way to find the parameters c,s \\in \\R c,s \\in \\R of the linear model f(z) = s\\cdot z + c f(z) = s\\cdot z + c that coresponds to a line of best fit is to minimize the following squared distance subject to a linear constraint: \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} The above minimization program can be reformulated as a linear least squares problem: \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, where \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. Let \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} The gradient and hessian of \\func{f}(\\vx) \\func{f}(\\vx) are: \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, respectively. The Hessian is positive semidefinite for every \\vx \\vx (and is positive definite if \\mA \\mA has full row rank). This implies that the function \\func{f}(\\vx) \\func{f}(\\vx) is convex. Additionally, \\vx = \\vx^{*} \\vx = \\vx^{*} is a critical point if \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} Since \\func{f}(\\vx) \\func{f}(\\vx) is convex, \\vx^{*} \\vx^{*} is a global minimizer. Equation \\eqref{least_square_normal_eqn} is called the normal equations of the least squares problem \\eqref{least_squares_problem}. Solving the normal equations, we get the following line of best fit.","title":"Least squares for data fitting"},{"location":"notes/Least_squares/#linear-systems","text":"Consider the problem of solving a linear system of equations. For \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in\\R^{m} \\vb\\in\\R^{m} , the linear system of equations \\mA\\vx = \\vb \\mA\\vx = \\vb is: overdetermined if m>n m>n , underdetermined if m< n m< n , or square if m = n m = n . A linear system can have exactly one solution, many solutions, or no solutions: In general, a linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has a solution if \\vb \\in \\text{range} (\\mA) \\vb \\in \\text{range} (\\mA) .","title":"Linear systems"},{"location":"notes/Least_squares/#properties-of-linear-least-squares","text":"Recall that the minimizer \\vx^* \\vx^* to the linear least squares poblem satisfies the normal equations: \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb with the residual \\vr^* = \\mA\\vx^* -\\vb, \\vr^* = \\mA\\vx^* -\\vb, satisfying \\mA^\\intercal\\vr^* = \\vzero. \\mA^\\intercal\\vr^* = \\vzero. Here, \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The minimzer of the linear least squares problem is unique if \\mA^\\intercal\\mA \\mA^\\intercal\\mA is invertible. However, the vector in the range of \\mA \\mA closest to \\vb \\vb is unique, i.e. \\vb^* = \\mA\\vx* \\vb^* = \\mA\\vx* is unique. Recall that range space of \\mA \\mA and the null space of \\mA^\\intercal \\mA^\\intercal is: \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} By fundamental theorem of linear algebra, we have $$ \\begin{equation}\\label{least_squares_FTLA} \\set{R}(\\mA) \\oplus \\set{N}(\\mA^\\intercal) = \\R^m. \\end{equation} $$ Thus, for all \\vx \\in \\R^m \\vx \\in \\R^m , we have \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) with \\vu \\vu and \\vv \\vv uniquely determined. This is illustrated in the figure below: Here, \\vx_{LS} \\vx_{LS} is the least squares solution, \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} , with \\va_i \\in \\R^m \\va_i \\in \\R^m for all i i . Comparing with \\eqref{least_squares_FTLA}, we get \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\exa{1} \\exa{1} What is the least-squares solution \\vx^* \\vx^* for the problem \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, where \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\text{Solution:} \\text{Solution:} First setup the normal equations: \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb Solving the normal equations, we get \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb So, the least squares solution \\vx^* \\vx^* is the mean value of the elements in \\vb \\vb .","title":"Properties of linear least squares"},{"location":"notes/QR_factorization/","text":"QR factorization \u00b6 Orthogonal and orthonormal vectors \u00b6 Let \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n be any two vectors. By cosine identity, we have \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) where, \\theta \\theta is the angle between \\vx \\vx and \\vy \\vy . So, \\vx \\vx and \\vy \\vy are orthogonal ( \\theta = 0 \\theta = 0 ), if \\vx\\trans\\vy = 0 \\vx\\trans\\vy = 0 . Furthermore, we say \\vx \\vx and \\vy \\vy are orthonomal if \\vx \\vx and \\vy \\vy have unit 2-norm and are orthogonal, i.e. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1. Orthogonal matrices \u00b6 A matrix \\mQ \\mQ is orthogonal if it is square and its columns are all pairwise orthogonal. For an orthogonal matrix \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat , we have \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. Since, a matrix \\mB \\mB is the inverse of a matrix \\mA \\mA if \\mB\\mA =\\mA\\mB = \\mI \\mB\\mA =\\mA\\mB = \\mI , the inverse of an orthognal matrix is its transpose, i.e. \\mQ^{-1} = \\mQ\\trans. \\mQ^{-1} = \\mQ\\trans. Orthogonal matrices have many good properties. One such property is that inner products are invariant under orthogonal transfromations. So, for any vectors \\vx,\\ \\vy \\vx,\\ \\vy , we have (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. This also implies that 2-norm is invariant to orthogonal transformations as \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 . Another property of othogonal matrices is that their determinant is either 1 1 or -1 -1 . This can be observed from the fact that for a matrix \\mA \\mA and \\mB \\mB , we have \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) and \\det(\\mA)= \\det(\\mA\\trans) \\det(\\mA)= \\det(\\mA\\trans) . So, from \\mQ\\trans\\mQ = \\mI \\mQ\\trans\\mQ = \\mI , we get \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11. \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11. QR factorization \u00b6 Let \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . A factorization of \\mA \\mA with \\mA = \\mQ \\mR \\mA = \\mQ \\mR where \\mQ\\in\\R^{m\\times m} \\mQ\\in\\R^{m\\times m} is an orthogonal matrix and \\mR \\in \\R^{m\\times n} \\mR \\in \\R^{m\\times n} is an upper triangular matrix is called a QR QR factorization of \\mA \\mA . In the case with m\\geq n m\\geq n and \\rank(\\mA) = k \\leq n \\rank(\\mA) = k \\leq n , the QR QR facorization will have the following shape: In the above figure, \\mQ \\mQ is an orthogonal matrix. \\mR \\mR is a upper triangular matrix, i.e. R_{ij}=0 R_{ij}=0 whenever i>j i>j . \\hat{\\mQ} \\hat{\\mQ} spans the range of \\mA \\mA . \\bar{\\mQ} \\bar{\\mQ} spans the nullspace of \\mA\\trans \\mA\\trans . In Julia, we can compute the QR decompisition of a matrix using: m = 4 n = 3 A = randn ( m , n ) F = qr ( A ) Let \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat . We can express the columns of \\mA \\mA as \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} So, we can compactly write the matrix \\mA \\mA as \\mA = \\hat{\\mQ}\\hat{\\mR} \\mA = \\hat{\\mQ}\\hat{\\mR} . This is the reduced (thin or economode) QR factorization of \\mA \\mA . In the case when m\\geq n m\\geq n and \\mA \\mA is full rank, we get following figure: Solving least squares via QR \u00b6 The QR QR factorization can be used to solve the least squares problem \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in \\R^m \\vb\\in \\R^m . We consider the case where m\\geq n m\\geq n , but QR factorization can be used to solve the other case as well. Consider \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} So, minimizing \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 will minimize \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 . In the case \\mA \\mA is full rank, we get an invertible \\hat{\\mR} \\hat{\\mR} and the least squares solution which satisfies \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} will be unique. In the case when \\mA \\mA is not full rank, there will be a infinitely many solutions to the least squares problem. For both cases, we can find a least squares solution by solving \\eqref{QR_leastsquares} via back substitution. The figure below shows the geometric prespective of using a QR factorization to solve the least squares problem. For every \\vb \\in \\R^m \\vb \\in \\R^m , \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb is the orthogonal projection of \\vb \\vb onto the \\range(\\hat{\\mQ}) = \\range(\\mA) \\range(\\hat{\\mQ}) = \\range(\\mA) . The least squares solution finds a point \\vx \\vx such that \\mA\\vx \\mA\\vx is equal tothe orthogonal projection of \\vb \\vb onto the \\range(\\mA) \\range(\\mA) . So, we get \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb , which simplifies to \\eqref{QR_leastsquares}.","title":"QR factorization"},{"location":"notes/QR_factorization/#qr-factorization","text":"","title":"QR factorization"},{"location":"notes/QR_factorization/#orthogonal-and-orthonormal-vectors","text":"Let \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n be any two vectors. By cosine identity, we have \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) where, \\theta \\theta is the angle between \\vx \\vx and \\vy \\vy . So, \\vx \\vx and \\vy \\vy are orthogonal ( \\theta = 0 \\theta = 0 ), if \\vx\\trans\\vy = 0 \\vx\\trans\\vy = 0 . Furthermore, we say \\vx \\vx and \\vy \\vy are orthonomal if \\vx \\vx and \\vy \\vy have unit 2-norm and are orthogonal, i.e. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1.","title":"Orthogonal and orthonormal vectors"},{"location":"notes/QR_factorization/#orthogonal-matrices","text":"A matrix \\mQ \\mQ is orthogonal if it is square and its columns are all pairwise orthogonal. For an orthogonal matrix \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat , we have \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. Since, a matrix \\mB \\mB is the inverse of a matrix \\mA \\mA if \\mB\\mA =\\mA\\mB = \\mI \\mB\\mA =\\mA\\mB = \\mI , the inverse of an orthognal matrix is its transpose, i.e. \\mQ^{-1} = \\mQ\\trans. \\mQ^{-1} = \\mQ\\trans. Orthogonal matrices have many good properties. One such property is that inner products are invariant under orthogonal transfromations. So, for any vectors \\vx,\\ \\vy \\vx,\\ \\vy , we have (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. This also implies that 2-norm is invariant to orthogonal transformations as \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 . Another property of othogonal matrices is that their determinant is either 1 1 or -1 -1 . This can be observed from the fact that for a matrix \\mA \\mA and \\mB \\mB , we have \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) and \\det(\\mA)= \\det(\\mA\\trans) \\det(\\mA)= \\det(\\mA\\trans) . So, from \\mQ\\trans\\mQ = \\mI \\mQ\\trans\\mQ = \\mI , we get \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11. \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11.","title":"Orthogonal matrices"},{"location":"notes/QR_factorization/#qr-factorization_1","text":"Let \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . A factorization of \\mA \\mA with \\mA = \\mQ \\mR \\mA = \\mQ \\mR where \\mQ\\in\\R^{m\\times m} \\mQ\\in\\R^{m\\times m} is an orthogonal matrix and \\mR \\in \\R^{m\\times n} \\mR \\in \\R^{m\\times n} is an upper triangular matrix is called a QR QR factorization of \\mA \\mA . In the case with m\\geq n m\\geq n and \\rank(\\mA) = k \\leq n \\rank(\\mA) = k \\leq n , the QR QR facorization will have the following shape: In the above figure, \\mQ \\mQ is an orthogonal matrix. \\mR \\mR is a upper triangular matrix, i.e. R_{ij}=0 R_{ij}=0 whenever i>j i>j . \\hat{\\mQ} \\hat{\\mQ} spans the range of \\mA \\mA . \\bar{\\mQ} \\bar{\\mQ} spans the nullspace of \\mA\\trans \\mA\\trans . In Julia, we can compute the QR decompisition of a matrix using: m = 4 n = 3 A = randn ( m , n ) F = qr ( A ) Let \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat . We can express the columns of \\mA \\mA as \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} So, we can compactly write the matrix \\mA \\mA as \\mA = \\hat{\\mQ}\\hat{\\mR} \\mA = \\hat{\\mQ}\\hat{\\mR} . This is the reduced (thin or economode) QR factorization of \\mA \\mA . In the case when m\\geq n m\\geq n and \\mA \\mA is full rank, we get following figure:","title":"QR factorization"},{"location":"notes/QR_factorization/#solving-least-squares-via-qr","text":"The QR QR factorization can be used to solve the least squares problem \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in \\R^m \\vb\\in \\R^m . We consider the case where m\\geq n m\\geq n , but QR factorization can be used to solve the other case as well. Consider \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} So, minimizing \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 will minimize \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 . In the case \\mA \\mA is full rank, we get an invertible \\hat{\\mR} \\hat{\\mR} and the least squares solution which satisfies \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} will be unique. In the case when \\mA \\mA is not full rank, there will be a infinitely many solutions to the least squares problem. For both cases, we can find a least squares solution by solving \\eqref{QR_leastsquares} via back substitution. The figure below shows the geometric prespective of using a QR factorization to solve the least squares problem. For every \\vb \\in \\R^m \\vb \\in \\R^m , \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb is the orthogonal projection of \\vb \\vb onto the \\range(\\hat{\\mQ}) = \\range(\\mA) \\range(\\hat{\\mQ}) = \\range(\\mA) . The least squares solution finds a point \\vx \\vx such that \\mA\\vx \\mA\\vx is equal tothe orthogonal projection of \\vb \\vb onto the \\range(\\mA) \\range(\\mA) . So, we get \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb , which simplifies to \\eqref{QR_leastsquares}.","title":"Solving least squares via QR"},{"location":"notes/background/","text":"Mathematical background \u00b6 Vectors \u00b6 Vectors are arrays of numbers ordered as a column. For example, a vector \\vx \\in \\R^n \\vx \\in \\R^n contain n n entries with x_i x_i as its i i th entry: \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. Some special vectors that are commonly used are the zero vector, ones vector, and the standard basis vectors. These are defined next: Zero vector: A vector \\vx \\vx is the zero vector if all of its entries are zero. The zero vector is denoted by \\vzero \\vzero . Ones vector: A vector \\vx \\vx is the ones vector if all of its entries are one. The ones vector is denoted by \\ve \\ve . i i th standard basis vector: A vector \\vx \\in \\R^n \\vx \\in \\R^n is the i i th standard basis vector if x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} . The i i th standard basis vector is denoted by \\ve_i \\ve_i . For example, in \\R^3 \\R^3 , the 2 2 nd standard basis vector is \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} . A vector \\vx \\in \\R^n \\vx \\in \\R^n is equal to another vector \\vy \\in \\R^n \\vy \\in \\R^n if x_i = y_i x_i = y_i for all i \\in \\{1,2,\\dots,n\\} i \\in \\{1,2,\\dots,n\\} . Similarly, if all of the entries of a vector \\vx \\vx is equal to a scalar c \\in \\R c \\in \\R , then we write \\vx = c \\vx = c . These notions can be extended to inequalites in the following way: A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx \\geq c \\vx \\geq c if and only if x_i \\geq c x_i \\geq c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx > c \\vx > c if and only if x_i > c x_i > c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Block vector is a concise representation of a vector obtained by stacking vectors. For example, if \\vx \\in \\R^m \\vx \\in \\R^m , \\vy \\in \\R^n \\vy \\in \\R^n , and \\vz \\in \\R^p \\vz \\in \\R^p , then \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} is a block vector (also called stacked vector or conncatenated vector) obtained by stacking \\vx \\vx , \\vy \\vy , and \\vz \\vz vertically. This block vector \\va \\va can also be written as \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} . If the dimension of the vectors are equal (i.e. m = n = p m = n = p ), then these vector can be stacked horizontally as well. This will produce a matrix \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} where \\vx,\\vy \\vx,\\vy , and \\vz \\vz are the first, second, and third columns of \\mB \\mB , respectively. Matrices \u00b6 A matrix is an array of numbers and is another way of collecting numbers. For example, a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} contain n n rows and m m columns with a total of m\\cdot n m\\cdot n number of entries: \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. The tranpose of this matrix \\mA \\mA is denoted by \\mA^\\intercal\\in\\R^{n\\times m} \\mA^\\intercal\\in\\R^{n\\times m} where \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} A matrix \\mA \\mA is the zero matrix if all of its entrie are zero. The zero matrix is denoted by \\mzero \\mzero . For a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} , if m>n m>n then the matrix \\mA \\mA is a tall matrix, if m< n m< n then the matrix \\mA \\mA is a wide matrix, and if m = n m = n then the matrix \\mA \\mA is a square matrix. Matrices \\mA \\mA and \\mB \\mB are equal if these matrices are of same size and every element is equal: $$ \\mA = \\mB \\iff A_{ij} = B_{ij},\\quad \\forall i = 1,\\dots,m, \\quad j = 1,\\dots,n. $$ If all of the entries of a matrix \\mA \\mA are a scalar c \\in \\R c \\in \\R then we write \\mA = c \\mA = c . These notions are extended to inequalities in the following way: \\mA \\geq c \\mA \\geq c if and only if A_{ij} \\geq c A_{ij} \\geq c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > c \\mA > c if and only if A_{ij} > c A_{ij} > c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA \\geq \\mB \\mA \\geq \\mB if and only if these matrices are of same size and A_{ij} \\geq B_{ij} A_{ij} \\geq B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > \\mB \\mA > \\mB if and only if these matrices are of same size and A_{ij} > B_{ij} A_{ij} > B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. Inner products \u00b6 Inner product is a way to multiply two vectors and matrices to produce a scalar. For \\setV = \\R^n\\text{ or } \\R^{m\\times n} \\setV = \\R^n\\text{ or } \\R^{m\\times n} , inner product is a map \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R that satisfies the following properties for all \\vx, \\vy, \\vz \\in \\setV \\vx, \\vy, \\vz \\in \\setV and \\alpha \\in \\R \\alpha \\in \\R : Non-negativity: \\langle \\vx,\\vx\\rangle \\geq 0 \\langle \\vx,\\vx\\rangle \\geq 0 and \\langle \\vx,\\vx\\rangle = 0 \\langle \\vx,\\vx\\rangle = 0 if and only if \\vx = 0 \\vx = 0 . Symmetry: \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle . Linearity: \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle and \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle . For \\setV = \\R^n \\setV = \\R^n , the Euclidean inner product between two vector \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n is also denoted as \\vx^\\intercal\\vy \\vx^\\intercal\\vy and \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\exa{1} \\exa{1} Fix i \\in \\{1,\\dots,n\\} i \\in \\{1,\\dots,n\\} . The inner product of i i th standard basis vector \\ve_i \\ve_i with an arbitary vector \\vx \\vx is the i i th entry of \\vx \\vx , i.e. \\ve_i^\\intercal\\vx = x_i \\ve_i^\\intercal\\vx = x_i . \\exa{2} \\exa{2} The inner product of the vector of ones \\ve \\ve with an arbitary vector \\vx \\vx is in the sum of entres of \\vx \\vx , i.e. \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n . \\exa{3} \\exa{3} The inner product on an arbitary vector \\vx \\vx with itself is the sum of squares of the entries of \\vx \\vx , i.e. \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 . \\exa{4} \\exa{4} Let \\vp = (p_1,p_2,\\dots,p_n) \\vp = (p_1,p_2,\\dots,p_n) be a vector of prices and \\vq = (q_1,q_2,\\dots,q_n) \\vq = (q_1,q_2,\\dots,q_n) be the corresponding vector of quantities. The total cost is the inner product of \\vp \\vp and \\vq \\vq , i.e. \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i . \\exa{5} \\exa{5} Let \\vw \\vw be a vector of assest allocations ( \\ve^\\intercal\\vw = 1, \\vw\\geq 0 \\ve^\\intercal\\vw = 1, \\vw\\geq 0 ) and \\vp \\vp be the corresponding vector of asset prices. The total portfolio value is \\vw^\\intercal\\vp \\vw^\\intercal\\vp . For \\setV = \\R^{m\\times n} \\setV = \\R^{m\\times n} , the trace inner product between two matrices \\mA, \\mB \\in \\R^{m\\times n} \\mA, \\mB \\in \\R^{m\\times n} is also denoted as \\text{tr}(\\mA^\\intercal\\mB) \\text{tr}(\\mA^\\intercal\\mB) and \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, where for a square matrix \\mS \\in \\R^{n\\times n} \\mS \\in \\R^{n\\times n} , \\trace{\\mS} =\\sum_{i=1}^n S_{ii} \\trace{\\mS} =\\sum_{i=1}^n S_{ii} , and for a matrix \\mA \\mA with columns \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m , \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}. \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}. Norms \u00b6 Norms are a measure of distances of a vector from the origin. The most common norm is the \"Euclidean norm\", i.e. 2-norm: \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} This 2-norm is the norm induced by the Euclidean inner product on \\R^n \\R^n , where \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} . The 2-norm reults in the familiar pythagorean identity, i.e. for any vectors \\va, \\vb, \\va, \\vb, and \\vc \\vc , \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. An important inequlity that relates the inner product of two arbitary vectors with the norms of those vectors induced by the inner product is the Cauchy-Schwartz inequality. In the case of \\R^n \\R^n , the Cauchy-Schwartz inequality states that for all \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n , we have \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. For \\R^n \\R^n , the Cauchy Schwartz inequality follows from the Cosine inequality, i.e. for any vectors \\vx, \\vy \\vx, \\vy with angle \\theta \\theta between \\vx \\vx and \\vy \\vy , we have \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) . So, the Cauchy-Schwartz inequality attains the equality if and only if the two vectors are co-linear. The Cauchy-Schwartz inequality gives us another inequailty that norms, in general, satisfy. This inequailty is the triangle inequality and states that \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\proof \\proof We will use Cauchy-Schwartz inequality to prove \\eqref{background:triangle}. Consider \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} where the second equality follows from linearty of inner product and the inequality follows from the Cauchy-Schwartz inequality. q.e.d. More generally, norms are any function \\|\\cdot\\|:\\R^n \\rightarrow \\R \\|\\cdot\\|:\\R^n \\rightarrow \\R that satisfy the following conditions for all \\vx,\\vy \\in\\R^n \\vx,\\vy \\in\\R^n and \\beta \\in \\R \\beta \\in \\R : Homogeneity: \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| . Triangle inequality: \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| . Non-negativity: \\|\\vx\\|\\geq 0 \\|\\vx\\|\\geq 0 and \\|\\vx\\|=0 \\|\\vx\\|=0 if and only if \\vx = \\vzero \\vx = \\vzero . Other useful norms that satify the above conditions are: 1-norm: The 1-norm of \\vx \\vx is \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| . Sup-norm: The sup-norm of \\vx \\vx is the largest entry of \\vx \\vx in absolute value, i.e. \\|\\vx\\|_\\infty = \\max_{j}|x_j| \\|\\vx\\|_\\infty = \\max_{j}|x_j| . p-norm: For p\\geq 1 p\\geq 1 , the p-norm of \\vx \\vx is \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} . Linear functions \u00b6 Linear function in a mapping between two Eucliean spaces that preserve the operations of addition and scalar multiplication. Specifially, a function \\func{f}:\\R^n\\rightarrow\\R^m \\func{f}:\\R^n\\rightarrow\\R^m is linear if for all \\vx,\\vy\\in\\R^n \\vx,\\vy\\in\\R^n and \\alpha\\in\\R \\alpha\\in\\R , we have \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) Here, \\func{f} \\func{f} takes a vector of dimension n n as its input and outputs a vector of dimension of m m . If m=1 m=1 , then we say the function \\func{f} \\func{f} is a real valued function. \\prop \\prop A real valued function \\func{f}:\\R^n\\rightarrow \\R \\func{f}:\\R^n\\rightarrow \\R is linear if and only if \\func{f} = \\va^\\intercal\\vx \\func{f} = \\va^\\intercal\\vx for some \\va \\in\\R^n \\va \\in\\R^n . \\proof \\proof We will first prove the forward direction, i.e. if \\func{f}:\\R^n\\rightarrow\\R \\func{f}:\\R^n\\rightarrow\\R is a linear function then \\func{f}(\\vx)=\\va^\\intercal\\vx \\func{f}(\\vx)=\\va^\\intercal\\vx . For i\\in\\{1,\\dots,n\\} i\\in\\{1,\\dots,n\\} , let a_{i} = \\func{f}(\\ve_i) a_{i} = \\func{f}(\\ve_i) . Then since \\vx = x_1\\ve_1+\\dots+x_n\\ve_n \\vx = x_1\\ve_1+\\dots+x_n\\ve_n , we have \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} Second, we prove the reverse direction, i.e. if \\func{f}(\\vx) = \\va^\\intercal\\vx \\func{f}(\\vx) = \\va^\\intercal\\vx for some \\va \\in \\R^n \\va \\in \\R^n then \\func{f} \\func{f} is a real valued linear function. Clearly, \\func{f}(x) \\func{f}(x) is a real valued function. Let \\vx,\\vy\\in \\R^n \\vx,\\vy\\in \\R^n and \\alpha \\in \\R \\alpha \\in \\R . Consider \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} where the second inequality follows from linearity of inner product. q.e.d.","title":"Mathematical background"},{"location":"notes/background/#mathematical-background","text":"","title":"Mathematical background"},{"location":"notes/background/#vectors","text":"Vectors are arrays of numbers ordered as a column. For example, a vector \\vx \\in \\R^n \\vx \\in \\R^n contain n n entries with x_i x_i as its i i th entry: \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. Some special vectors that are commonly used are the zero vector, ones vector, and the standard basis vectors. These are defined next: Zero vector: A vector \\vx \\vx is the zero vector if all of its entries are zero. The zero vector is denoted by \\vzero \\vzero . Ones vector: A vector \\vx \\vx is the ones vector if all of its entries are one. The ones vector is denoted by \\ve \\ve . i i th standard basis vector: A vector \\vx \\in \\R^n \\vx \\in \\R^n is the i i th standard basis vector if x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} . The i i th standard basis vector is denoted by \\ve_i \\ve_i . For example, in \\R^3 \\R^3 , the 2 2 nd standard basis vector is \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} . A vector \\vx \\in \\R^n \\vx \\in \\R^n is equal to another vector \\vy \\in \\R^n \\vy \\in \\R^n if x_i = y_i x_i = y_i for all i \\in \\{1,2,\\dots,n\\} i \\in \\{1,2,\\dots,n\\} . Similarly, if all of the entries of a vector \\vx \\vx is equal to a scalar c \\in \\R c \\in \\R , then we write \\vx = c \\vx = c . These notions can be extended to inequalites in the following way: A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx \\geq c \\vx \\geq c if and only if x_i \\geq c x_i \\geq c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx > c \\vx > c if and only if x_i > c x_i > c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Block vector is a concise representation of a vector obtained by stacking vectors. For example, if \\vx \\in \\R^m \\vx \\in \\R^m , \\vy \\in \\R^n \\vy \\in \\R^n , and \\vz \\in \\R^p \\vz \\in \\R^p , then \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} is a block vector (also called stacked vector or conncatenated vector) obtained by stacking \\vx \\vx , \\vy \\vy , and \\vz \\vz vertically. This block vector \\va \\va can also be written as \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} . If the dimension of the vectors are equal (i.e. m = n = p m = n = p ), then these vector can be stacked horizontally as well. This will produce a matrix \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} where \\vx,\\vy \\vx,\\vy , and \\vz \\vz are the first, second, and third columns of \\mB \\mB , respectively.","title":"Vectors"},{"location":"notes/background/#matrices","text":"A matrix is an array of numbers and is another way of collecting numbers. For example, a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} contain n n rows and m m columns with a total of m\\cdot n m\\cdot n number of entries: \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. The tranpose of this matrix \\mA \\mA is denoted by \\mA^\\intercal\\in\\R^{n\\times m} \\mA^\\intercal\\in\\R^{n\\times m} where \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} A matrix \\mA \\mA is the zero matrix if all of its entrie are zero. The zero matrix is denoted by \\mzero \\mzero . For a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} , if m>n m>n then the matrix \\mA \\mA is a tall matrix, if m< n m< n then the matrix \\mA \\mA is a wide matrix, and if m = n m = n then the matrix \\mA \\mA is a square matrix. Matrices \\mA \\mA and \\mB \\mB are equal if these matrices are of same size and every element is equal: $$ \\mA = \\mB \\iff A_{ij} = B_{ij},\\quad \\forall i = 1,\\dots,m, \\quad j = 1,\\dots,n. $$ If all of the entries of a matrix \\mA \\mA are a scalar c \\in \\R c \\in \\R then we write \\mA = c \\mA = c . These notions are extended to inequalities in the following way: \\mA \\geq c \\mA \\geq c if and only if A_{ij} \\geq c A_{ij} \\geq c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > c \\mA > c if and only if A_{ij} > c A_{ij} > c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA \\geq \\mB \\mA \\geq \\mB if and only if these matrices are of same size and A_{ij} \\geq B_{ij} A_{ij} \\geq B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > \\mB \\mA > \\mB if and only if these matrices are of same size and A_{ij} > B_{ij} A_{ij} > B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n.","title":"Matrices"},{"location":"notes/background/#inner-products","text":"Inner product is a way to multiply two vectors and matrices to produce a scalar. For \\setV = \\R^n\\text{ or } \\R^{m\\times n} \\setV = \\R^n\\text{ or } \\R^{m\\times n} , inner product is a map \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R that satisfies the following properties for all \\vx, \\vy, \\vz \\in \\setV \\vx, \\vy, \\vz \\in \\setV and \\alpha \\in \\R \\alpha \\in \\R : Non-negativity: \\langle \\vx,\\vx\\rangle \\geq 0 \\langle \\vx,\\vx\\rangle \\geq 0 and \\langle \\vx,\\vx\\rangle = 0 \\langle \\vx,\\vx\\rangle = 0 if and only if \\vx = 0 \\vx = 0 . Symmetry: \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle . Linearity: \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle and \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle . For \\setV = \\R^n \\setV = \\R^n , the Euclidean inner product between two vector \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n is also denoted as \\vx^\\intercal\\vy \\vx^\\intercal\\vy and \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\exa{1} \\exa{1} Fix i \\in \\{1,\\dots,n\\} i \\in \\{1,\\dots,n\\} . The inner product of i i th standard basis vector \\ve_i \\ve_i with an arbitary vector \\vx \\vx is the i i th entry of \\vx \\vx , i.e. \\ve_i^\\intercal\\vx = x_i \\ve_i^\\intercal\\vx = x_i . \\exa{2} \\exa{2} The inner product of the vector of ones \\ve \\ve with an arbitary vector \\vx \\vx is in the sum of entres of \\vx \\vx , i.e. \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n . \\exa{3} \\exa{3} The inner product on an arbitary vector \\vx \\vx with itself is the sum of squares of the entries of \\vx \\vx , i.e. \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 . \\exa{4} \\exa{4} Let \\vp = (p_1,p_2,\\dots,p_n) \\vp = (p_1,p_2,\\dots,p_n) be a vector of prices and \\vq = (q_1,q_2,\\dots,q_n) \\vq = (q_1,q_2,\\dots,q_n) be the corresponding vector of quantities. The total cost is the inner product of \\vp \\vp and \\vq \\vq , i.e. \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i . \\exa{5} \\exa{5} Let \\vw \\vw be a vector of assest allocations ( \\ve^\\intercal\\vw = 1, \\vw\\geq 0 \\ve^\\intercal\\vw = 1, \\vw\\geq 0 ) and \\vp \\vp be the corresponding vector of asset prices. The total portfolio value is \\vw^\\intercal\\vp \\vw^\\intercal\\vp . For \\setV = \\R^{m\\times n} \\setV = \\R^{m\\times n} , the trace inner product between two matrices \\mA, \\mB \\in \\R^{m\\times n} \\mA, \\mB \\in \\R^{m\\times n} is also denoted as \\text{tr}(\\mA^\\intercal\\mB) \\text{tr}(\\mA^\\intercal\\mB) and \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, where for a square matrix \\mS \\in \\R^{n\\times n} \\mS \\in \\R^{n\\times n} , \\trace{\\mS} =\\sum_{i=1}^n S_{ii} \\trace{\\mS} =\\sum_{i=1}^n S_{ii} , and for a matrix \\mA \\mA with columns \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m , \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}. \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}.","title":"Inner products"},{"location":"notes/background/#norms","text":"Norms are a measure of distances of a vector from the origin. The most common norm is the \"Euclidean norm\", i.e. 2-norm: \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} This 2-norm is the norm induced by the Euclidean inner product on \\R^n \\R^n , where \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} . The 2-norm reults in the familiar pythagorean identity, i.e. for any vectors \\va, \\vb, \\va, \\vb, and \\vc \\vc , \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. An important inequlity that relates the inner product of two arbitary vectors with the norms of those vectors induced by the inner product is the Cauchy-Schwartz inequality. In the case of \\R^n \\R^n , the Cauchy-Schwartz inequality states that for all \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n , we have \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. For \\R^n \\R^n , the Cauchy Schwartz inequality follows from the Cosine inequality, i.e. for any vectors \\vx, \\vy \\vx, \\vy with angle \\theta \\theta between \\vx \\vx and \\vy \\vy , we have \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) . So, the Cauchy-Schwartz inequality attains the equality if and only if the two vectors are co-linear. The Cauchy-Schwartz inequality gives us another inequailty that norms, in general, satisfy. This inequailty is the triangle inequality and states that \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\proof \\proof We will use Cauchy-Schwartz inequality to prove \\eqref{background:triangle}. Consider \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} where the second equality follows from linearty of inner product and the inequality follows from the Cauchy-Schwartz inequality. q.e.d. More generally, norms are any function \\|\\cdot\\|:\\R^n \\rightarrow \\R \\|\\cdot\\|:\\R^n \\rightarrow \\R that satisfy the following conditions for all \\vx,\\vy \\in\\R^n \\vx,\\vy \\in\\R^n and \\beta \\in \\R \\beta \\in \\R : Homogeneity: \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| . Triangle inequality: \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| . Non-negativity: \\|\\vx\\|\\geq 0 \\|\\vx\\|\\geq 0 and \\|\\vx\\|=0 \\|\\vx\\|=0 if and only if \\vx = \\vzero \\vx = \\vzero . Other useful norms that satify the above conditions are: 1-norm: The 1-norm of \\vx \\vx is \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| . Sup-norm: The sup-norm of \\vx \\vx is the largest entry of \\vx \\vx in absolute value, i.e. \\|\\vx\\|_\\infty = \\max_{j}|x_j| \\|\\vx\\|_\\infty = \\max_{j}|x_j| . p-norm: For p\\geq 1 p\\geq 1 , the p-norm of \\vx \\vx is \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} .","title":"Norms"},{"location":"notes/background/#linear-functions","text":"Linear function in a mapping between two Eucliean spaces that preserve the operations of addition and scalar multiplication. Specifially, a function \\func{f}:\\R^n\\rightarrow\\R^m \\func{f}:\\R^n\\rightarrow\\R^m is linear if for all \\vx,\\vy\\in\\R^n \\vx,\\vy\\in\\R^n and \\alpha\\in\\R \\alpha\\in\\R , we have \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) Here, \\func{f} \\func{f} takes a vector of dimension n n as its input and outputs a vector of dimension of m m . If m=1 m=1 , then we say the function \\func{f} \\func{f} is a real valued function. \\prop \\prop A real valued function \\func{f}:\\R^n\\rightarrow \\R \\func{f}:\\R^n\\rightarrow \\R is linear if and only if \\func{f} = \\va^\\intercal\\vx \\func{f} = \\va^\\intercal\\vx for some \\va \\in\\R^n \\va \\in\\R^n . \\proof \\proof We will first prove the forward direction, i.e. if \\func{f}:\\R^n\\rightarrow\\R \\func{f}:\\R^n\\rightarrow\\R is a linear function then \\func{f}(\\vx)=\\va^\\intercal\\vx \\func{f}(\\vx)=\\va^\\intercal\\vx . For i\\in\\{1,\\dots,n\\} i\\in\\{1,\\dots,n\\} , let a_{i} = \\func{f}(\\ve_i) a_{i} = \\func{f}(\\ve_i) . Then since \\vx = x_1\\ve_1+\\dots+x_n\\ve_n \\vx = x_1\\ve_1+\\dots+x_n\\ve_n , we have \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} Second, we prove the reverse direction, i.e. if \\func{f}(\\vx) = \\va^\\intercal\\vx \\func{f}(\\vx) = \\va^\\intercal\\vx for some \\va \\in \\R^n \\va \\in \\R^n then \\func{f} \\func{f} is a real valued linear function. Clearly, \\func{f}(x) \\func{f}(x) is a real valued function. Let \\vx,\\vy\\in \\R^n \\vx,\\vy\\in \\R^n and \\alpha \\in \\R \\alpha \\in \\R . Consider \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} where the second inequality follows from linearity of inner product. q.e.d.","title":"Linear functions"},{"location":"notes/introduction/","text":"Introduction \u00b6 This is the introduction to the class. We will cover unconstrained and constrained optimization. Modelling \u00b6 This is how you model. See eq 1 Let's create another reference to optimimality","title":"Introduction"},{"location":"notes/introduction/#introduction","text":"This is the introduction to the class. We will cover unconstrained and constrained optimization.","title":"Introduction"},{"location":"notes/introduction/#modelling","text":"This is how you model. See eq 1 Let's create another reference to optimimality","title":"Modelling"},{"location":"notes/unconstrained/","text":"Unconstrained Optimzation \u00b6 In this lecture we consider unconstrained optimization","title":"**Unconstrained Optimzation**"},{"location":"notes/unconstrained/#unconstrained-optimzation","text":"In this lecture we consider unconstrained optimization","title":"Unconstrained Optimzation"},{"location":"notes/constrained/","text":"Constrained optimization \u00b6 These are the new notes","title":"Constrained optimization"},{"location":"notes/constrained/#constrained-optimization","text":"These are the new notes","title":"Constrained optimization"}]}